{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brwana/Chatbot/blob/main/Chatbot%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSGXHp6DRjmd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intent-based dataset"
      ],
      "metadata": {
        "id": "7PHuInlrgZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/Intent.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data.keys())  # Should show: dict_keys(['intents'])\n"
      ],
      "metadata": {
        "id": "nmJpkbcGWUUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "intents = data['intents']\n",
        "pairs = []\n",
        "\n",
        "for intent in intents:\n",
        "    inputs = intent['text']\n",
        "    responses = intent['responses']\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Pick a random response for each input\n",
        "        response = random.choice(responses)\n",
        "        pairs.append({\"input\": input_text, \"output\": response})\n"
      ],
      "metadata": {
        "id": "9MjIkJFTXGoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "1IJLMG6FXPnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Daily Dialogue Dataset"
      ],
      "metadata": {
        "id": "ms6Z4jtYjKt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dialogue_files = [\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_train.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_validation.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw//dialogues_test.txt'\n",
        "# ]\n",
        "\n",
        "# dialogue_text = \"\"\n",
        "\n",
        "# for file_path in dialogue_files:\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         dialogue_text += f.read() + \"\\n\"\n",
        "\n",
        "# # Step 3: Convert dialogue to input-output pairs\n",
        "# dialogue_lines = dialogue_text.split('__eou__')\n",
        "# dialogue_lines = [line.strip() for line in dialogue_lines if line.strip()]\n",
        "\n",
        "# for i in range(len(dialogue_lines) - 1):\n",
        "#     input_line = dialogue_lines[i]\n",
        "#     output_line = dialogue_lines[i + 1]\n",
        "#     pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "\n",
        "# # Step 4: Shuffle all pairs\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # (Optional) Preview a few\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {pairs[i]['input']} \\nA: {pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "jZTuZKIZjSOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import ast\n",
        "\n",
        "# # === Step 1: Load all lines into a dictionary ===\n",
        "# def load_movie_lines(file_path):\n",
        "#     id2line = {}\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 5:\n",
        "#                 line_id = parts[0]\n",
        "#                 text = parts[4]\n",
        "#                 id2line[line_id] = text\n",
        "#     return id2line\n",
        "\n",
        "# # === Step 2: Parse conversations ===\n",
        "# def load_conversations(file_path):\n",
        "#     conversations = []\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 4:\n",
        "#                 # literal_eval is used to convert the string list to actual list\n",
        "#                 line_ids = ast.literal_eval(parts[3])\n",
        "#                 conversations.append(line_ids)\n",
        "#     return conversations\n",
        "\n",
        "# # === Step 3: Extract input-output pairs ===\n",
        "# def create_pairs(id2line, conversations):\n",
        "#     qa_pairs = []\n",
        "#     for conv in conversations:\n",
        "#         for i in range(len(conv) - 1):\n",
        "#             input_line = id2line.get(conv[i], \"\")\n",
        "#             output_line = id2line.get(conv[i + 1], \"\")\n",
        "#             if input_line and output_line:\n",
        "#                 qa_pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "#     return qa_pairs\n",
        "\n",
        "# # === File paths ===\n",
        "# lines_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_lines.txt'\n",
        "# convs_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_conversations.txt'\n",
        "\n",
        "# # === Run the processing ===\n",
        "# id2line = load_movie_lines(lines_path)\n",
        "# conversations = load_conversations(convs_path)\n",
        "# movie_pairs = create_pairs(id2line, conversations)\n",
        "\n",
        "# # === Preview a few ===\n",
        "# print(f\"Total pairs: {len(movie_pairs)}\")\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {movie_pairs[i]['input']} \\nA: {movie_pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "6Mf-30nzmJp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Combine all ===\n",
        "# pairs.extend(movie_pairs)\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # === Save to JSON ===\n",
        "# output_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs Cleaned/combined_pairs.json'\n",
        "# with open(output_path, 'w', encoding='utf-8') as f:\n",
        "#     json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(f\"Saved {len(pairs)} input-output pairs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "hyWgeFH2nGOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Data Preparation"
      ],
      "metadata": {
        "id": "XnL9LNmxpCul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Extract input and output texts\n",
        "input_texts = [pair[\"input\"] for pair in pairs]\n",
        "output_texts = [pair[\"output\"] for pair in pairs]\n",
        "\n",
        "# Add start and end tokens for output (for seq2seq model)\n",
        "output_texts = [\"<start> \" + text + \" <end>\" for text in output_texts]\n",
        "\n",
        "# Tokenizers\n",
        "input_tokenizer = Tokenizer(filters='')\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "output_tokenizer = Tokenizer(filters='')\n",
        "output_tokenizer.fit_on_texts(output_texts)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "encoder_input_data = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "decoder_input_data = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# Decoder target data is same as decoder input but shifted one step to the left\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n"
      ],
      "metadata": {
        "id": "V_mqDdyNNEQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# Parameters\n",
        "encoder_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(output_tokenizer.word_index) + 1\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(encoder_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb = Embedding(decoder_vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(decoder_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare target with one extra dimension\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "# Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=16,\n",
        "          epochs=200,\n",
        "          validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "joHots9CRWmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate([X_test_enc, X_test_dec], y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Hb3WG4KfUhgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([X_test_enc, X_test_dec])\n",
        "predicted_word_indices = np.argmax(predictions, axis=-1)\n"
      ],
      "metadata": {
        "id": "8-2RQFwRUq0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count = 0\n",
        "total = len(X_test_enc)\n",
        "for i in range(total):\n",
        "    pred_seq = predicted_word_indices[i]\n",
        "    true_seq = y_test[i].squeeze()\n",
        "    if np.array_equal(pred_seq, true_seq):\n",
        "        correct_count += 1\n",
        "print(\"Exact Match Accuracy:\", correct_count / total)\n"
      ],
      "metadata": {
        "id": "2et1W5hsU2R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n"
      ],
      "metadata": {
        "id": "2Q9Fqrb9VDFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs for the decoder at inference time\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb  # reuse the same embedding layer\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2\n",
        ")\n"
      ],
      "metadata": {
        "id": "5whg50jcVFOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence with just the start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = output_tokenizer.word_index['<start>']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = output_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_output_len:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "            # Update the target sequence (of length 1)\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "aGaRd7M5VG3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(user_input):\n",
        "    seq = input_tokenizer.texts_to_sequences([user_input])\n",
        "    seq = pad_sequences(seq, maxlen=max_input_len, padding='post')\n",
        "    response = decode_sequence(seq)\n",
        "    return response\n",
        "\n",
        "# Test the chatbot\n",
        "while True:\n",
        "    inp = input(\"You: \")\n",
        "    if inp.lower() in ['quit', 'exit']:\n",
        "        break\n",
        "    print(\"Bot:\", respond(inp))\n"
      ],
      "metadata": {
        "id": "fFhgYOhNVIM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "id": "K3l_0Ab5NmWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autointent"
      ],
      "metadata": {
        "id": "oboIUPfoRIj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers"
      ],
      "metadata": {
        "id": "QDZrbJuhRmSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SNIPS dataset\n",
        "snips = load_dataset(\"benayas/snips\")\n"
      ],
      "metadata": {
        "id": "5XWJYI6vNsNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(snips['train'][0])  # Print the first example in the train split to check the columns\n"
      ],
      "metadata": {
        "id": "x_pEo0JERu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autointent.schemas import Intent\n",
        "\n",
        "def _extract_intents_data(split):\n",
        "    # Extract unique intent names\n",
        "    intent_names = sorted(split.unique(\"category\"))\n",
        "    # Map intent names to unique IDs\n",
        "    name_to_id = dict(zip(intent_names, range(len(intent_names))))\n",
        "\n",
        "    # Create Intent objects\n",
        "    intents_data = [Intent(id=i, name=name) for i, name in enumerate(intent_names)]\n",
        "\n",
        "    return name_to_id, intents_data\n",
        "\n",
        "# Extract intents from the training split\n",
        "name_to_id, intents_data = _extract_intents_data(snips['train'])\n"
      ],
      "metadata": {
        "id": "_cQI3bZYRyEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autointent import Dataset\n",
        "from autointent.schemas import Sample\n",
        "\n",
        "def convert_snips(split, name_to_id):\n",
        "    n_classes = len(name_to_id)\n",
        "    classwise_samples = [[] for _ in range(n_classes)]\n",
        "\n",
        "    for batch in split.iter(batch_size=16, drop_last_batch=False):\n",
        "        for txt, name in zip(batch[\"text\"], batch[\"category\"]):\n",
        "            intent_id = name_to_id[name]\n",
        "            target_list = classwise_samples[intent_id]\n",
        "            target_list.append({\"utterance\": txt, \"label\": intent_id})\n",
        "\n",
        "    return [Sample(**sample) for samples_from_one_class in classwise_samples for sample in samples_from_one_class]\n",
        "\n",
        "# Convert train and test splits\n",
        "train_samples = convert_snips(snips['train'], name_to_id)\n",
        "test_samples = convert_snips(snips['test'], name_to_id)\n"
      ],
      "metadata": {
        "id": "0xKArH0xR28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_dict({\n",
        "    \"train\": train_samples,\n",
        "    \"test\": test_samples,\n",
        "    \"intents\": intents_data\n",
        "})\n",
        "\n",
        "# Print the final dataset\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "Kd9r-RUzR5GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[\"train\"][:5])  # Print first 5 samples in the train set\n"
      ],
      "metadata": {
        "id": "aKtHE-ZsR7Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Load the Snips dataset\n",
        "# dataset = load_dataset(\"DeepPavlov/snips\")\n",
        "\n",
        "# # Explore the dataset\n",
        "# print(dataset)\n",
        "# print(dataset['train'][0])\n"
      ],
      "metadata": {
        "id": "BxttCHpxN23_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# # Tokenizer setup\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts([x['utterance'] for x in dataset['train']])\n",
        "\n",
        "# # Convert texts to padded sequences\n",
        "# X_train = tokenizer.texts_to_sequences([x['utterance'] for x in dataset['train']])\n",
        "# X_test = tokenizer.texts_to_sequences([x['utterance'] for x in dataset['test']])\n",
        "# X_train = pad_sequences(X_train)\n",
        "# X_test = pad_sequences(X_test, maxlen=X_train.shape[1])\n"
      ],
      "metadata": {
        "id": "bx_z4g4TN4sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Extract intent labels\n",
        "# # Extract intent labels from the 'label' key\n",
        "# y_train = [x['label'] for x in dataset['train']]\n",
        "# y_test = [x['label'] for x in dataset['test']]\n",
        "\n",
        "\n",
        "# # Encode labels to integers\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train = label_encoder.fit_transform(y_train)\n",
        "# y_test = label_encoder.transform(y_test)\n",
        "\n",
        "# # Number of unique classes (intents)\n",
        "# num_classes = len(label_encoder.classes_)\n",
        "# print(\"Intent classes:\", label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "46h0dtv2ObXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training samples (utterances)\n",
        "tokenizer.fit_on_texts([sample.utterance for sample in train_samples])\n",
        "\n",
        "# Convert training and testing data to sequences\n",
        "X_train = tokenizer.texts_to_sequences([sample.utterance for sample in train_samples])\n",
        "X_test = tokenizer.texts_to_sequences([sample.utterance for sample in test_samples])\n",
        "\n",
        "# Pad the sequences to the same length\n",
        "max_sequence_length = 50  # Set the maximum sequence length for input sequences\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length)\n",
        "X_test = pad_sequences(X_test, maxlen=max_sequence_length)\n",
        "\n",
        "# Prepare the labels\n",
        "y_train = [sample.label for sample in train_samples]\n",
        "y_test = [sample.label for sample in test_samples]\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the number of classes (intents)\n",
        "num_classes = len(name_to_id)  # Number of intent classes\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,  # Vocabulary size\n",
        "                    output_dim=100,  # Size of word embeddings\n",
        "                    input_length=max_sequence_length))  # Length of input sequences\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # LSTM layer with dropout\n",
        "model.add(Dense(64, activation='relu'))  # Fully connected layer\n",
        "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy for integer labels\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Build the model with input shape\n",
        "model.build(input_shape=(None, max_sequence_length))\n",
        "\n",
        "# Now you can see the summary of the model\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "Vvj8m3kLOyiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "nlbXmdLgO0gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Import numpy\n",
        "\n",
        "def predict_intent(utterance):\n",
        "    # Convert the new utterance to sequence\n",
        "    new_sequence = tokenizer.texts_to_sequences([utterance])\n",
        "    new_sequence_padded = pad_sequences(new_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "    # Predict the intent\n",
        "    predicted_class = model.predict(new_sequence_padded)\n",
        "    predicted_label = np.argmax(predicted_class, axis=1)\n",
        "\n",
        "    # Map predicted label back to intent name\n",
        "    predicted_intent = list(name_to_id.keys())[list(name_to_id.values()).index(predicted_label[0])]\n",
        "    return predicted_intent\n"
      ],
      "metadata": {
        "id": "vL83MJNSPN94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assume the following variables are already defined:\n",
        "# - tokenizer: The tokenizer used for text preprocessing\n",
        "# - model: The trained LSTM model\n",
        "# - name_to_id: The mapping from intent names to IDs\n",
        "# - intents_data: List of intents, each containing a name\n",
        "\n",
        "# Mapping of intent indices to response (this can be adjusted based on your specific intents)\n",
        "intent_responses = {\n",
        "    0: 'Sure! I can help you book a flight. Where would you like to go?',\n",
        "    1: 'I can help with the weather. What city are you in?',\n",
        "    2: 'I can play music for you! What genre would you like?',\n",
        "    3: 'I can help with restaurant searches. What cuisine are you interested in?',\n",
        "    4: 'I can help with your calendar. What event would you like to add?',\n",
        "    5: 'I can help with alarms. When would you like to set one?',\n",
        "    6: 'I can help with reminders. What should I remind you about?'\n",
        "}\n",
        "\n",
        "def predict_intent(utterance):\n",
        "    # Tokenize and pad the user input to match the model's input requirements\n",
        "    sequence = tokenizer.texts_to_sequences([utterance])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=50)  # Ensure consistent sequence length\n",
        "\n",
        "    # Predict the intent using the trained model\n",
        "    prediction = model.predict(padded_sequence)\n",
        "\n",
        "    # Get the predicted intent (the label with the highest probability)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Start the interaction loop\n",
        "print(\"Hello! I am your chatbot. How can I assist you today?\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() in ['exit', 'quit']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Predict the intent using the trained model\n",
        "    predicted_intent = predict_intent(user_input)\n",
        "\n",
        "    # Get the response based on the predicted intent\n",
        "    response = intent_responses.get(predicted_intent, \"Sorry, I didn't understand that.\")\n",
        "\n",
        "    # Print the bot's response\n",
        "    print(f\"Bot: {response}\")\n"
      ],
      "metadata": {
        "id": "FPRWIL8LQQbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install datasets autointent transformers numpy tensorflow\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from autointent import Dataset\n",
        "from autointent.schemas import Intent, Sample\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# ------------------------\n",
        "# 1. Data Loading & Preparation\n",
        "# ------------------------\n",
        "\n",
        "# Load the SNIPS dataset\n",
        "snips = load_dataset(\"benayas/snips\")\n",
        "\n",
        "# Clean text data\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove special chars\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to all samples\n",
        "for split in ['train', 'test']:\n",
        "    snips[split] = snips[split].map(lambda x: {\"text\": clean_text(x[\"text\"]), \"category\": x[\"category\"]})\n",
        "\n",
        "# Extract intents\n",
        "def _extract_intents_data(split):\n",
        "    intent_names = sorted(split.unique(\"category\"))\n",
        "    name_to_id = dict(zip(intent_names, range(len(intent_names))))\n",
        "    intents_data = [Intent(id=i, name=name) for i, name in enumerate(intent_names)]\n",
        "    return name_to_id, intents_data\n",
        "\n",
        "name_to_id, intents_data = _extract_intents_data(snips['train'])\n",
        "\n",
        "# Convert dataset to AutoIntent format\n",
        "def convert_snips(split, name_to_id):\n",
        "    classwise_samples = defaultdict(list)\n",
        "    for txt, name in zip(split[\"text\"], split[\"category\"]):\n",
        "        intent_id = name_to_id[name]\n",
        "        classwise_samples[intent_id].append({\"utterance\": txt, \"label\": intent_id})\n",
        "    return [Sample(**sample) for samples in classwise_samples.values() for sample in samples]\n",
        "\n",
        "train_samples = convert_snips(snips['train'], name_to_id)\n",
        "test_samples = convert_snips(snips['test'], name_to_id)\n",
        "\n",
        "# ------------------------\n",
        "# 2. Text Preprocessing\n",
        "# ------------------------\n",
        "\n",
        "# Initialize and fit tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([sample.utterance for sample in train_samples])\n",
        "\n",
        "# Vocabulary size and max sequence length\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_sequence_length = max(len(seq) for seq in tokenizer.texts_to_sequences([sample.utterance for sample in train_samples]))\n",
        "\n",
        "# Prepare sequences\n",
        "def prepare_sequences(samples):\n",
        "    sequences = tokenizer.texts_to_sequences([sample.utterance for sample in samples])\n",
        "    padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "    labels = [sample.label for sample in samples]\n",
        "    return padded, np.array(labels)\n",
        "\n",
        "X_train, y_train = prepare_sequences(train_samples)\n",
        "X_test, y_test = prepare_sequences(test_samples)\n",
        "\n",
        "# ------------------------\n",
        "# 3. Model Architecture\n",
        "# ------------------------\n",
        "\n",
        "# Improved model with bidirectional LSTM\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length, mask_zero=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(name_to_id), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 4. Model Training\n",
        "# ------------------------\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# ------------------------\n",
        "# 5. Prediction & Chat Interface\n",
        "# ------------------------\n",
        "\n",
        "# Complete intent responses\n",
        "intent_responses = {\n",
        "    0: 'Sure! I can help you book a flight. Where would you like to go?',\n",
        "    1: 'I can help with the weather. What city are you in?',\n",
        "    2: 'I can play music for you! What genre would you like?',\n",
        "    3: 'I can help with restaurant searches. What cuisine are you interested in?',\n",
        "    4: 'I can help with your calendar. What event would you like to add?',\n",
        "    5: 'I can help with alarms. When would you like to set one?',\n",
        "    6: 'I can help with reminders. What should I remind you about?'\n",
        "}\n",
        "\n",
        "def predict_intent(utterance, confidence_threshold=0.7):\n",
        "    # Clean and prepare input\n",
        "    cleaned = clean_text(utterance)\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
        "    if not sequence[0]:  # No recognizable words\n",
        "        return None, 0.0\n",
        "\n",
        "    padded = pad_sequences(sequence, maxlen=max_sequence_length, padding='post', truncating='post')\n",
        "\n",
        "    # Get prediction probabilities\n",
        "    prediction = model.predict(padded, verbose=0)[0]\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    confidence = prediction[predicted_class]\n",
        "\n",
        "    return (predicted_class, confidence) if confidence >= confidence_threshold else (None, confidence)\n",
        "\n",
        "# Chat interface\n",
        "print(\"Hello! I am your improved chatbot. How can I assist you today? (Type 'exit' to quit)\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"Bot: Goodbye! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        print(\"Bot: Please type something so I can help you!\")\n",
        "        continue\n",
        "\n",
        "    intent, confidence = predict_intent(user_input)\n",
        "\n",
        "    if intent is None:\n",
        "        print(f\"Bot: I'm not confident I understood that (confidence: {confidence:.2f}). Could you rephrase?\")\n",
        "    else:\n",
        "        print(f\"Bot: {intent_responses[intent]} (confidence: {confidence:.2f})\")"
      ],
      "metadata": {
        "id": "DqKjLdbXQahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. First, let's adjust our preprocessing and model training\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Analyze word frequencies to ensure proper tokenization\n",
        "all_words = [word for sample in train_samples for word in sample.utterance.split()]\n",
        "word_freq = Counter(all_words)\n",
        "print(\"Most common words:\", word_freq.most_common(20))\n",
        "\n",
        "# 2. Update the model architecture with better hyperparameters\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length, mask_zero=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))),\n",
        "    Bidirectional(LSTM(64, kernel_regularizer=l2(0.01))),\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    Dropout(0.3),  # Reduced from 0.5 to prevent over-regularization\n",
        "    Dense(len(name_to_id), activation='softmax')\n",
        "])\n",
        "\n",
        "# Use a more sophisticated optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)  # Reduced learning rate\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 3. Add data augmentation by repeating minority classes\n",
        "def balance_classes(samples):\n",
        "    class_counts = Counter([s.label for s in samples])\n",
        "    max_count = max(class_counts.values())\n",
        "\n",
        "    balanced_samples = []\n",
        "    for class_id in class_counts:\n",
        "        class_samples = [s for s in samples if s.label == class_id]\n",
        "        # Oversample minority classes\n",
        "        balanced_samples.extend(class_samples * (max_count // len(class_samples)))\n",
        "        balanced_samples.extend(class_samples[:max_count % len(class_samples)])\n",
        "\n",
        "    return balanced_samples\n",
        "\n",
        "balanced_train_samples = balance_classes(train_samples)\n",
        "X_train, y_train = prepare_sequences(balanced_train_samples)\n",
        "\n",
        "# 4. Train with class weights to handle any remaining imbalance\n",
        "class_weights = {i: 1./count for i, count in Counter(y_train).items()}\n",
        "class_weights = {k: v/min(class_weights.values()) for k,v in class_weights.items()}\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Increased epochs\n",
        "    batch_size=64,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# 5. Update the prediction function with better defaults\n",
        "def predict_intent(utterance, confidence_threshold=0.5):  # Lowered threshold\n",
        "    cleaned = clean_text(utterance)\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
        "\n",
        "    if not sequence[0]:\n",
        "        # Handle unknown words by looking for keywords\n",
        "        keywords = {\n",
        "            'book': 0, 'flight': 0, 'fly': 0,\n",
        "            'weather': 1, 'temperature': 1,\n",
        "            'music': 2, 'play': 2, 'song': 2,\n",
        "            'restaurant': 3, 'eat': 3, 'food': 3,\n",
        "            'calendar': 4, 'event': 4, 'meeting': 4,\n",
        "            'alarm': 5, 'wake': 5, 'remind': 6\n",
        "        }\n",
        "        matches = [keywords[word] for word in cleaned.split() if word in keywords]\n",
        "        if matches:\n",
        "            predicted_class = Counter(matches).most_common(1)[0][0]\n",
        "            return predicted_class, 0.65  # Medium confidence for keyword matches\n",
        "        return None, 0.0\n",
        "\n",
        "    padded = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "    prediction = model.predict(padded, verbose=0)[0]\n",
        "    predicted_class = np.argmax(prediction)\n",
        "    confidence = prediction[predicted_class]\n",
        "\n",
        "    # Boost confidence for clear matches\n",
        "    if confidence > 0.3 and prediction[predicted_class] > 2*np.mean(prediction):\n",
        "        confidence = min(1.0, confidence*1.3)\n",
        "\n",
        "    return (predicted_class, confidence) if confidence >= confidence_threshold else (None, confidence)\n",
        "\n",
        "# 6. Improved chat interface with fallback responses\n",
        "intent_keywords = {\n",
        "    0: ['book', 'flight', 'travel', 'ticket'],\n",
        "    1: ['weather', 'forecast', 'temperature'],\n",
        "    2: ['music', 'play', 'song', 'artist'],\n",
        "    3: ['restaurant', 'food', 'eat', 'dinner'],\n",
        "    4: ['calendar', 'event', 'meeting', 'schedule'],\n",
        "    5: ['alarm', 'wake', 'timer'],\n",
        "    6: ['remind', 'reminder', 'remember']\n",
        "}\n",
        "\n",
        "def get_fallback_response(text):\n",
        "    text = clean_text(text)\n",
        "    word_scores = {class_id:0 for class_id in intent_responses}\n",
        "\n",
        "    for word in text.split():\n",
        "        for class_id, keywords in intent_keywords.items():\n",
        "            if word in keywords:\n",
        "                word_scores[class_id] += 1\n",
        "\n",
        "    best_class = max(word_scores.items(), key=lambda x: x[1])[0]\n",
        "    if word_scores[best_class] > 0:\n",
        "        return intent_responses[best_class]\n",
        "    return \"Could you provide more details about what you need help with?\"\n",
        "\n",
        "print(\"Hello! I'm your enhanced chatbot. How can I help you today?\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "\n",
        "    if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"Bot: Goodbye! Safe travels!\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        print(\"Bot: I'm here to help! What would you like assistance with?\")\n",
        "        continue\n",
        "\n",
        "    intent, confidence = predict_intent(user_input)\n",
        "\n",
        "    if intent is not None:\n",
        "        print(f\"Bot: {intent_responses[intent]} (confidence: {confidence:.2f})\")\n",
        "    else:\n",
        "        fallback = get_fallback_response(user_input)\n",
        "        print(f\"Bot: {fallback} (I'm {confidence:.2f} confident in this response)\")"
      ],
      "metadata": {
        "id": "tHth5t74Y4QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xL6jUKEydaYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "snips = load_dataset(\"benayas/snips\")\n",
        "train_data = snips['train']\n",
        "test_data = snips['test']\n"
      ],
      "metadata": {
        "id": "giVa68d9delB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_data['category'])\n",
        "test_labels = label_encoder.transform(test_data['category'])\n",
        "\n",
        "num_labels = len(label_encoder.classes_)\n"
      ],
      "metadata": {
        "id": "q6eEqPjhdggK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "train_encodings = tokenizer(train_data['text'], truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_data['text'], truncation=True, padding=True)\n"
      ],
      "metadata": {
        "id": "jpXGipesdiNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class IntentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = IntentDataset(train_encodings, train_labels)\n",
        "test_dataset = IntentDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "2eEeB4QOdlWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "m3PWUt5HdnNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gMnTCjNuduja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "id": "bM14QO5md2Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Create basic arguments first\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=10,\n",
        "    logging_dir=\"./logs\"\n",
        ")\n",
        "\n",
        "# Then SET the evaluation/save parameters using the correct attribute names\n",
        "if hasattr(training_args, 'evaluation_strategy'):  # Newer versions\n",
        "    training_args.evaluation_strategy = \"epoch\"\n",
        "    training_args.save_strategy = \"epoch\"\n",
        "elif hasattr(training_args, 'eval_strategy'):  # Older versions\n",
        "    training_args.eval_strategy = \"epoch\"\n",
        "    training_args.save_strategy = \"epoch\"  # or save_steps if needed\n",
        "\n",
        "# Required for load_best_model_at_end\n",
        "training_args.load_best_model_at_end = True\n",
        "if hasattr(training_args, 'metric_for_best_model'):\n",
        "    training_args.metric_for_best_model = \"accuracy\"\n",
        "    training_args.greater_is_better = True\n",
        "\n",
        "print(\"Final training arguments:\")\n",
        "print(training_args)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KSg9wlFUdpaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import DatasetDict\n",
        "import numpy as np\n",
        "\n",
        "## 1. Dataset Preparation ##\n",
        "def prepare_datasets(dataset_dict, tokenizer, max_length=128):\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "\n",
        "    # Convert text labels to numerical IDs\n",
        "    def map_labels(examples):\n",
        "        return {\"labels\": [name_to_id[category] for category in examples[\"category\"]]}\n",
        "\n",
        "    tokenized_datasets = tokenized_datasets.map(map_labels, batched=True)\n",
        "    tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    return tokenized_datasets\n",
        "\n",
        "# Create validation split if needed\n",
        "if 'validation' not in snips:\n",
        "    snips = DatasetDict({\n",
        "        'train': snips['train'].train_test_split(test_size=0.1)['train'],\n",
        "        'validation': snips['train'].train_test_split(test_size=0.1)['test']\n",
        "    })\n",
        "\n",
        "# Process datasets\n",
        "tokenized_datasets = prepare_datasets(snips, tokenizer)\n",
        "\n",
        "## 2. Universal Trainer Setup ##\n",
        "def get_trainer(model):\n",
        "    # Common arguments\n",
        "    args = {\n",
        "        \"output_dir\": \"./results\",\n",
        "        \"num_train_epochs\": 3,\n",
        "        \"per_device_train_batch_size\": 16,\n",
        "        \"per_device_eval_batch_size\": 16,\n",
        "        \"logging_steps\": 10,\n",
        "        \"load_best_model_at_end\": True,\n",
        "        \"metric_for_best_model\": \"accuracy\",\n",
        "        \"greater_is_better\": True,\n",
        "        \"report_to\": \"none\",\n",
        "        \"remove_unused_columns\": False,\n",
        "        \"save_total_limit\": 2\n",
        "    }\n",
        "\n",
        "    # Version-specific configuration\n",
        "    if hasattr(TrainingArguments, 'evaluation_strategy'):\n",
        "        # Modern versions (4.0+)\n",
        "        args.update({\n",
        "            \"evaluation_strategy\": \"epoch\",\n",
        "            \"save_strategy\": \"epoch\",\n",
        "            \"logging_strategy\": \"steps\"\n",
        "        })\n",
        "    else:\n",
        "        # Legacy versions (pre-4.0)\n",
        "        args.update({\n",
        "            \"evaluate_during_training\": True,\n",
        "            \"eval_steps\": None,  # Epoch-based evaluation\n",
        "            \"save_steps\": None,  # Epoch-based saving\n",
        "            \"logging_steps\": 10\n",
        "        })\n",
        "\n",
        "    # Create training arguments\n",
        "    training_args = TrainingArguments(**args)\n",
        "\n",
        "    # For legacy versions, manually set epoch-based behavior\n",
        "    if not hasattr(TrainingArguments, 'evaluation_strategy'):\n",
        "        training_args.eval_steps = None\n",
        "        training_args.save_steps = None\n",
        "\n",
        "    # Metric computation\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        accuracy = np.mean(predictions == labels)\n",
        "        return {\"accuracy\": accuracy}\n",
        "\n",
        "    return Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "## 3. Run Training ##\n",
        "try:\n",
        "    trainer = get_trainer(model)\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"Training failed: {str(e)}\")\n",
        "    print(\"Attempting fallback configuration...\")\n",
        "\n",
        "    # Fallback with minimal arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results_fallback\",\n",
        "        per_device_train_batch_size=16,\n",
        "        num_train_epochs=3\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"]\n",
        "    )\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "AhYiXTK3f9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def export_model(trainer, tokenizer, label_encoder):\n",
        "    # Create model directory\n",
        "    import os\n",
        "    os.makedirs(\"./intent_classifier\", exist_ok=True)\n",
        "\n",
        "    # Save model components\n",
        "    trainer.save_model(\"./intent_classifier\")\n",
        "    tokenizer.save_pretrained(\"./intent_classifier\")\n",
        "\n",
        "    # Save label encoder\n",
        "    with open(\"./intent_classifier/label_encoder.pkl\", \"wb\") as f:\n",
        "        pickle.dump(label_encoder, f)\n",
        "\n",
        "    print(\"Model exported successfully!\")\n",
        "\n",
        "# Call after training\n",
        "export_model(trainer, tokenizer, label_encoder)"
      ],
      "metadata": {
        "id": "JlPHl-ZMfbi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r intent_classifier.zip intent_classifier/"
      ],
      "metadata": {
        "id": "1K9zRPe7f8S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, reload your saved model and components\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./intent_classifier\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./intent_classifier\")\n",
        "\n",
        "# Load label encoder\n",
        "with open(\"./intent_classifier/label_encoder.pkl\", \"rb\") as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Define response templates\n",
        "# intent_responses = {\n",
        "#     \"BookFlight\": \"I can help book flights! ✈️ Where would you like to go?\",\n",
        "#     \"PlayMusic\": \"🎵 What song or artist would you like to hear?\",\n",
        "#     \"SearchCreativeWork\": \"Looking for books, movies or songs? Tell me what you're searching for.\",\n",
        "#     \"AddToPlaylist\": \"Let's add to your playlist. Which song?\",\n",
        "#     \"RateBook\": \"I can help rate books. Which title are you reviewing?\",\n",
        "#     \"GetWeather\": \"Want weather info? Please tell me your city.\",\n",
        "#     \"SearchScreeningEvent\": \"Looking for movie showtimes? What's your location?\"\n",
        "# }\n",
        "\n",
        "def predict_intent(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_id = outputs.logits.argmax().item()\n",
        "    return label_encoder.inverse_transform([predicted_id])[0]\n",
        "\n",
        "# Interactive chat loop\n",
        "# Enhanced version with greeting handling and fallback improvements\n",
        "print(\"🤖 Hello! I'm your intent classifier bot. Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \").strip()\n",
        "\n",
        "    # Handle exit command\n",
        "    if user_input.lower() in ['quit', 'exit']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Handle greetings and small talk\n",
        "    if user_input.lower() in ['hi', 'hello', 'hey']:\n",
        "        print(\"Bot: Hello! How can I help you today?\")\n",
        "        print(\"(Detected: Greeting)\")\n",
        "        continue\n",
        "\n",
        "    # Get prediction\n",
        "    intent = predict_intent(user_input)\n",
        "    confidence = torch.softmax(model(**tokenizer(user_input, return_tensors=\"pt\")).logits, dim=-1).max().item()\n",
        "\n",
        "    # Improved fallback logic\n",
        "    if confidence < 0.7:  # Only 70% confident\n",
        "        if any(word in user_input.lower() for word in ['book', 'flight']):\n",
        "            intent = \"BookFlight\"\n",
        "        elif any(word in user_input.lower() for word in ['play', 'music', 'song']):\n",
        "            intent = \"PlayMusic\"\n",
        "        else:\n",
        "            print(\"Bot: I'm not quite sure what you need. Could you be more specific?\")\n",
        "            print(f\"(Uncertain intent: {intent}, Confidence: {confidence:.0%})\")\n",
        "            continue\n",
        "\n",
        "    # Generate response\n",
        "    response = intent_responses.get(intent, \"I can help with that. Tell me more!\")\n",
        "    print(f\"Bot: {response}\")\n",
        "    print(f\"(Detected intent: {intent}, Confidence: {confidence:.0%})\\n\")"
      ],
      "metadata": {
        "id": "4c0QN_awgPvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional\n",
        "import re\n",
        "\n",
        "class HotelBookingChatbot:\n",
        "    def __init__(self):\n",
        "        # Initialize device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load a conversational model\n",
        "        self.model_name = \"facebook/blenderbot-400M-distill\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)\n",
        "\n",
        "        # Only focus on hotel booking for now\n",
        "        self.intent_strategies = {\n",
        "            \"hotel_booking\": {\n",
        "                \"description\": \"Handles hotel room bookings\",\n",
        "                \"required_info\": [\"location\", \"check_in_date\", \"check_out_date\", \"guests\"],\n",
        "                \"confirmation_phrases\": [\"book hotel\", \"reserve hotel\", \"hotel reservation\", \"hotel booking\", \"stay at hotel\"]\n",
        "            },\n",
        "            \"out_of_scope\": {\n",
        "                \"description\": \"Handles queries unrelated to hotels\",\n",
        "                \"required_info\": [],\n",
        "                \"confirmation_phrases\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.conversation_history = []\n",
        "        self.current_context = {\n",
        "            \"active_intent\": None,\n",
        "            \"collected_info\": {},\n",
        "            \"missing_info\": []\n",
        "        }\n",
        "\n",
        "    def classify_intent(self, user_input: str) -> str:\n",
        "        \"\"\"Classify intent based on keywords.\"\"\"\n",
        "        user_input = user_input.lower()\n",
        "        for intent, strategy in self.intent_strategies.items():\n",
        "            for phrase in strategy[\"confirmation_phrases\"]:\n",
        "                if phrase in user_input:\n",
        "                    return intent\n",
        "        # If it doesn't match, mark as out of scope\n",
        "        return \"out_of_scope\"\n",
        "\n",
        "    def extract_information(self, intent: str, user_input: str) -> Dict:\n",
        "        \"\"\"Extract hotel booking info from input.\"\"\"\n",
        "        extracted = {}\n",
        "\n",
        "        if intent == \"hotel_booking\":\n",
        "            if any(word in user_input.lower() for word in [\"cairo\", \"paris\", \"london\", \"new york\", \"dubai\"]):\n",
        "                extracted[\"location\"] = re.findall(r'\\b(cairo|paris|london|new york|dubai)\\b', user_input.lower())[0].title()\n",
        "            if \"check_in_date\" in self.intent_strategies[intent][\"required_info\"]:\n",
        "                extracted[\"check_in_date\"] = self._extract_date(user_input)\n",
        "            if \"check_out_date\" in self.intent_strategies[intent][\"required_info\"]:\n",
        "                extracted[\"check_out_date\"] = self._extract_date(user_input)\n",
        "            if \"guests\" in self.intent_strategies[intent][\"required_info\"]:\n",
        "                match = re.search(r'(\\d+) (guests|people|persons|adults|kids)', user_input.lower())\n",
        "                if match:\n",
        "                    extracted[\"guests\"] = int(match.group(1))\n",
        "\n",
        "        return extracted\n",
        "\n",
        "    def generate_response(self, intent: str, user_input: str) -> str:\n",
        "        \"\"\"Generate the next chatbot response.\"\"\"\n",
        "        if intent == \"out_of_scope\":\n",
        "            return \"I'm currently only able to assist with hotel bookings. Could you tell me if you'd like to book a hotel stay?\"\n",
        "\n",
        "        new_info = self.extract_information(intent, user_input)\n",
        "        self._update_context(intent, new_info)\n",
        "\n",
        "        if self._is_complete(intent):\n",
        "            return self._generate_completion_response(intent)\n",
        "        else:\n",
        "            return self._generate_next_question(intent)\n",
        "\n",
        "    def _update_context(self, intent: str, new_info: Dict):\n",
        "        \"\"\"Update the current conversation context.\"\"\"\n",
        "        if self.current_context[\"active_intent\"] != intent:\n",
        "            self.current_context = {\n",
        "                \"active_intent\": intent,\n",
        "                \"collected_info\": new_info,\n",
        "                \"missing_info\": self._get_missing_info(intent, new_info)\n",
        "            }\n",
        "        else:\n",
        "            self.current_context[\"collected_info\"].update(new_info)\n",
        "            self.current_context[\"missing_info\"] = self._get_missing_info(\n",
        "                intent, self.current_context[\"collected_info\"])\n",
        "\n",
        "    def _get_missing_info(self, intent: str, collected_info: Dict) -> List:\n",
        "        \"\"\"Check what information is missing.\"\"\"\n",
        "        return [info for info in self.intent_strategies[intent][\"required_info\"]\n",
        "                if info not in collected_info or collected_info[info] is None]\n",
        "\n",
        "    def _is_complete(self, intent: str) -> bool:\n",
        "        \"\"\"Check if enough info was collected.\"\"\"\n",
        "        return len(self.current_context[\"missing_info\"]) == 0\n",
        "\n",
        "    def _generate_next_question(self, intent: str) -> str:\n",
        "        \"\"\"Ask for the next missing piece of information.\"\"\"\n",
        "        missing_info = self.current_context[\"missing_info\"][0]\n",
        "        questions = {\n",
        "            \"location\": \"Where would you like to book the hotel?\",\n",
        "            \"check_in_date\": \"When is your check-in date?\",\n",
        "            \"check_out_date\": \"When is your check-out date?\",\n",
        "            \"guests\": \"How many guests will be staying?\"\n",
        "        }\n",
        "        return questions.get(missing_info, \"Could you provide more details about your hotel booking?\")\n",
        "\n",
        "    def _generate_completion_response(self, intent: str) -> str:\n",
        "        \"\"\"Response when all needed information is collected.\"\"\"\n",
        "        context = self.current_context[\"collected_info\"]\n",
        "        response = (\n",
        "            f\"Perfect! I've got you booked in {context.get('location')} \"\n",
        "            f\"from {context.get('check_in_date')} to {context.get('check_out_date')} \"\n",
        "            f\"for {context.get('guests')} guest(s). Would you like to proceed with the reservation?\"\n",
        "        )\n",
        "        self.current_context = {\n",
        "            \"active_intent\": None,\n",
        "            \"collected_info\": {},\n",
        "            \"missing_info\": []\n",
        "        }\n",
        "        return response\n",
        "\n",
        "    def _extract_date(self, text: str) -> Optional[str]:\n",
        "        \"\"\"Extract date from user input.\"\"\"\n",
        "        patterns = [\n",
        "            r\"(\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b)\",  # MM/DD/YYYY\n",
        "            r\"(\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\b \\d{1,2},? \\d{4})\",\n",
        "            r\"(\\bnext \\w+\\b)\",\n",
        "            r\"(\\btomorrow\\b)\",\n",
        "            r\"(\\btoday\\b)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "        return None\n",
        "\n",
        "    def process_message(self, user_input: str) -> str:\n",
        "        \"\"\"Process user input end-to-end.\"\"\"\n",
        "        intent = self.classify_intent(user_input)\n",
        "        response = self.generate_response(intent, user_input)\n",
        "\n",
        "        self.conversation_history.append({\n",
        "            \"user\": user_input,\n",
        "            \"bot\": response,\n",
        "            \"intent\": intent,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "        return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Initializing Hotel Booking Chatbot...\")\n",
        "    chatbot = HotelBookingChatbot()\n",
        "    print(\"\\nChatbot: Hello! I'm here to help you book a hotel. How can I assist you today?\")\n",
        "    print(\"(Type 'quit' to exit.)\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
        "            print(\"Chatbot: Thank you! Have a wonderful day!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = chatbot.process_message(user_input)\n",
        "            print(f\"Chatbot: {response}\")\n",
        "        except Exception as e:\n",
        "            print(\"Chatbot: Oops, I had trouble understanding. Could you rephrase?\")\n",
        "            print(f\"[Debug info: {e}]\")\n"
      ],
      "metadata": {
        "id": "_2VsfYJWRkxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets\n"
      ],
      "metadata": {
        "id": "flTpn71jYiWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "dFOm3JHpZrX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)\n"
      ],
      "metadata": {
        "id": "kYDZ0CCQ2I9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load intents\n",
        "with open('First Aid Intents Dataset.json') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "# Prepare dataset\n",
        "texts = []\n",
        "labels = []\n",
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for idx, intent in enumerate(intents['intents']):\n",
        "    label2id[intent['tag']] = idx\n",
        "    id2label[idx] = intent['tag']\n",
        "    for pattern in intent['patterns']:\n",
        "        texts.append(pattern)\n",
        "        labels.append(idx)\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess(example):\n",
        "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset = dataset.map(preprocess)\n",
        "\n",
        "# Load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id))\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"  # 👈 THIS disables wandb\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save mappings for later\n",
        "torch.save(label2id, \"label2id.pt\")\n",
        "torch.save(id2label, \"id2label.pt\")\n"
      ],
      "metadata": {
        "id": "ldgw2u1zZvLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./travelbot_model\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(\"./travelbot_model\")"
      ],
      "metadata": {
        "id": "_DV22-VVzNxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./results\")\n",
        "tokenizer.save_pretrained(\"./results\")\n"
      ],
      "metadata": {
        "id": "N7OhUda00bWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('./results')\n",
        "model = BertForSequenceClassification.from_pretrained('./results')\n",
        "\n",
        "# Load label mappings\n",
        "label2id = torch.load(\"label2id.pt\")\n",
        "id2label = torch.load(\"id2label.pt\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "zG9pJ8PFzY9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Get logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "\n",
        "    # Map id back to tag\n",
        "    predicted_tag = id2label[predicted_class_id]\n",
        "\n",
        "    return predicted_tag\n"
      ],
      "metadata": {
        "id": "xyCSokSY0rv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"How to treat a sprain?\"\n",
        "predicted_tag = predict(test_text)\n",
        "print(f\"Predicted intent: {predicted_tag}\")\n"
      ],
      "metadata": {
        "id": "bqIAn22L0ute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"What should I do if I get a cut?\",\n",
        "    \"Hello, is anyone there?\",\n",
        "    \"I have a sore throat, what should I do?\",\n",
        "    \"Goodbye\",\n",
        "    \"What medicine to apply for stings?\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Predicted Tag: {predict(sentence)}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "gsoIhcct10-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load intents\n",
        "with open('First Aid Intents Dataset.json') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "# Prepare raw texts and labels\n",
        "texts = []\n",
        "labels = []\n",
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for idx, intent in enumerate(intents['intents']):\n",
        "    label2id[intent['tag']] = idx\n",
        "    id2label[idx] = intent['tag']\n",
        "    for pattern in intent['patterns']:\n",
        "        texts.append(pattern)\n",
        "        labels.append(idx)\n",
        "\n",
        "# 🔥 Now you can split them\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Dataset\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess(example):\n",
        "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "# Load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id))\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,  # <--- 🛠️ Evaluate on separate data!\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save mappings\n",
        "torch.save(label2id, \"label2id.pt\")\n",
        "torch.save(id2label, \"id2label.pt\")\n"
      ],
      "metadata": {
        "id": "8vsupiY42l5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load intents\n",
        "with open('First Aid Intents Dataset.json') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "# Prepare raw texts and labels\n",
        "texts = []\n",
        "labels = []\n",
        "label2id = {}\n",
        "id2label = {}\n",
        "\n",
        "for idx, intent in enumerate(intents['intents']):\n",
        "    label2id[intent['tag']] = idx\n",
        "    id2label[idx] = intent['tag']\n",
        "    for pattern in intent['patterns']:\n",
        "        texts.append(pattern)\n",
        "        labels.append(idx)\n",
        "\n",
        "# 🔥 Now you can split them\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    texts, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to Dataset\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "test_dataset = Dataset.from_dict({\"text\": test_texts, \"label\": test_labels})\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def preprocess(example):\n",
        "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "# Load model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id))\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,  # <--- 🛠️ Evaluate on separate data!\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save mappings\n",
        "torch.save(label2id, \"label2id.pt\")\n",
        "torch.save(id2label, \"id2label.pt\")\n"
      ],
      "metadata": {
        "id": "1EKSlgLv34je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./results 2\")\n",
        "tokenizer.save_pretrained(\"./results 2\")\n"
      ],
      "metadata": {
        "id": "gVrMnVThLtq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('./results 2')\n",
        "model = BertForSequenceClassification.from_pretrained('./results 2')\n",
        "\n",
        "# Load label mappings\n",
        "label2id = torch.load(\"label2id.pt\")\n",
        "id2label = torch.load(\"id2label.pt\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "xcn6TpBaL03b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Get logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "\n",
        "    # Map id back to tag\n",
        "    predicted_tag = id2label[predicted_class_id]\n",
        "\n",
        "    return predicted_tag\n"
      ],
      "metadata": {
        "id": "-pWVD5EUMATj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"How to treat a sprain?\"\n",
        "predicted_tag = predict(test_text)\n",
        "print(f\"Predicted intent: {predicted_tag}\")\n"
      ],
      "metadata": {
        "id": "DvQddJhqMCcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"What should I do if I get a cut?\",\n",
        "    \"Hello, is anyone there?\",\n",
        "    \"I have a sore throat, what should I do?\",\n",
        "    \"Goodbye\",\n",
        "    \"What medicine to apply for stings?\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Predicted Tag: {predict(sentence)}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "metadata": {
        "id": "F6nldLL3MEzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get true labels\n",
        "true_labels = test_labels\n",
        "\n",
        "# Predict\n",
        "predicted_labels = []\n",
        "for text in test_texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)\n",
        "    pred_label_id = torch.argmax(probs, dim=1).item()\n",
        "    predicted_labels.append(pred_label_id)\n",
        "\n",
        "# Get only labels that appear in true or predicted\n",
        "labels_in_test = sorted(list(set(true_labels) | set(predicted_labels)))\n",
        "\n",
        "# Generate report\n",
        "print(classification_report(true_labels, predicted_labels, labels=labels_in_test, target_names=[id2label[i] for i in labels_in_test]))\n"
      ],
      "metadata": {
        "id": "DLKpKBOwMXj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Load your intents (from the dataset you provided)\n",
        "with open('First Aid Intents Dataset.json') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "# Create a dictionary of responses by tag\n",
        "responses = {intent['tag']: intent['responses'] for intent in intents['intents']}\n",
        "\n",
        "def chat():\n",
        "    print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            print(\"Bot: Goodbye! 👋\")\n",
        "            break\n",
        "\n",
        "        # Predict the intent (use your 'predict' function here)\n",
        "        predicted_tag = predict(user_input)\n",
        "\n",
        "        # Respond with a random response from the matched tag\n",
        "        if predicted_tag in responses:\n",
        "            bot_response = random.choice(responses[predicted_tag])\n",
        "            print(f\"Bot: {bot_response}\")\n",
        "        else:\n",
        "            print(\"Bot: Sorry, I don't understand that yet.\")\n",
        "\n",
        "# Call the chat function to start the conversation\n",
        "chat()\n"
      ],
      "metadata": {
        "id": "jY-mA6gvNqWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=len(labels))  # adjust labels count\n",
        "\n",
        "# Define the labels for your intents\n",
        "labels = [\"Cuts\", \"greeting\", \"goodbye\", \"Sprains\", \"Headache\", \"Concussion\", \"Sore Throat\"]  # Update with your labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(labels)  # Encode labels\n"
      ],
      "metadata": {
        "id": "MZ_9m_P_PMAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub\n"
      ],
      "metadata": {
        "id": "23LAp7ZkSSJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface-cli login"
      ],
      "metadata": {
        "id": "_4ETu1g2SVkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "0BHUqnj-TJTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import os\n",
        "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\"\n",
        "\n",
        "# Define models for each domain and task\n",
        "intent_classifier_medical = pipeline(\"zero-shot-classification\", model=\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "intent_classifier_service = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")  # Customer Service\n",
        "intent_classifier_tech = pipeline(\"zero-shot-classification\", model=\"t5-small\")  # Tech Support\n",
        "intent_classifier_banking = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\")\n",
        "\n",
        "# Define NER models\n",
        "ner_model_medical = pipeline(\"ner\", model=\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "ner_model_service = pipeline(\"ner\", model=\"bert-base-cased\")\n",
        "ner_model_tech = pipeline(\"ner\", model=\"t5-small\")  # Fine-tuned tech support NER\n",
        "ner_model_banking = pipeline(\"ner\", model=\"roberta-large-mnli\")  # Banking NER\n",
        "\n",
        "# Define response generation models\n",
        "response_generator_medical = pipeline(\"text-generation\", model=\"t5-small\")\n"
      ],
      "metadata": {
        "id": "5fBOsyVxRjpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_generator_service = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uPgRSAwVVyYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_generator_tech =response_generator_service  # Fine-tuned for tech support\n",
        "response_generator_banking =response_generator_service # Fine-tuned for banking responses\n",
        "\n",
        "# Sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "# Function to classify the domain\n",
        "def classify_domain(user_input):\n",
        "    if \"treatment\" in user_input or \"symptoms\" in user_input:\n",
        "        return \"Medical\"\n",
        "    elif \"order\" in user_input or \"payment\" in user_input or \"account\" in user_input:\n",
        "        return \"Customer Service\"\n",
        "    elif \"error\" in user_input or \"support\" in user_input or \"technical\" in user_input:\n",
        "        return \"Tech Support\"\n",
        "    elif \"balance\" in user_input or \"transaction\" in user_input:\n",
        "        return \"Banking\"\n",
        "    else:\n",
        "        return \"General\"\n",
        "\n",
        "# Function to generate a response\n",
        "def generate_response(user_input):\n",
        "    domain = classify_domain(user_input)\n",
        "    # Initialize variables\n",
        "    intent = None\n",
        "    entities = []\n",
        "    response = None\n",
        "\n",
        "\n",
        "    if domain == \"Medical\":\n",
        "        intent = intent_classifier_medical(user_input, candidate_labels=[\"Treatment\", \"Symptoms\", \"Prevention\"])\n",
        "        entities = ner_model_medical(user_input)\n",
        "        response = response_generator_medical(f\"Treatment for {entities[0]['word']}\")\n",
        "\n",
        "    elif domain == \"Customer Service\":\n",
        "        intent = intent_classifier_service(user_input, candidate_labels=[\"Order Status\", \"Payment Issues\", \"Account Help\"])\n",
        "        entities = ner_model_service(user_input)\n",
        "        response = response_generator_service(f\"Help with {entities[0]['word']}\")\n",
        "\n",
        "    elif domain == \"Tech Support\":\n",
        "        intent = intent_classifier_tech(user_input, candidate_labels=[\"Error\", \"Software Issues\", \"Account Setup\"])\n",
        "        entities = ner_model_tech(user_input)\n",
        "        response = response_generator_tech(f\"Support for {entities[0]['word']}\")\n",
        "\n",
        "    elif domain == \"Banking\":\n",
        "        intent = intent_classifier_banking(user_input, candidate_labels=[\"Balance Inquiry\", \"Transaction\", \"Account Issue\"])\n",
        "        entities = ner_model_banking(user_input)\n",
        "        response = response_generator_banking(f\"Banking assistance with {entities[0]['word']}\")\n",
        "\n",
        "    sentiment = sentiment_analyzer(user_input)\n",
        "\n",
        "    # Adjust the response based on sentiment\n",
        "    if sentiment[0]['label'] == 'NEGATIVE':\n",
        "        response = f\"I'm sorry you're feeling frustrated. Let's resolve this quickly. {response[0]['generated_text']}\"\n",
        "\n",
        "    return {\n",
        "        \"Domain\": domain,\n",
        "        \"Intent\": intent,\n",
        "        \"Entities\": entities,\n",
        "        \"Response\": response[0]['generated_text'],\n",
        "        \"Sentiment\": sentiment\n",
        "    }\n",
        "\n",
        "# Example interaction\n",
        "user_input = \"I need help with my order status.\"\n",
        "response = generate_response(user_input)\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "id": "80TKvm1sWW44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the individual models\n",
        "# intent_classifier_medical.model.save_pretrained(\"saved_models/intent_classifier_medical\")\n",
        "# intent_classifier_service.model.save_pretrained(\"saved_models/intent_classifier_service\")\n",
        "# intent_classifier_tech.model.save_pretrained(\"saved_models/intent_classifier_tech\")\n",
        "# intent_classifier_banking.model.save_pretrained(\"saved_models/intent_classifier_banking\")\n",
        "\n",
        "# # Similarly for the other models\n",
        "# response_generator_medical.model.save_pretrained(\"saved_models/response_generator_medical\")\n",
        "# response_generator_service.model.save_pretrained(\"saved_models/response_generator_service\")\n",
        "# response_generator_tech.model.save_pretrained(\"saved_models/response_generator_tech\")\n",
        "# response_generator_banking.model.save_pretrained(\"saved_models/response_generator_banking\")\n"
      ],
      "metadata": {
        "id": "JUJMst2FXAaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    response = generate_response(user_input)\n",
        "\n",
        "    # Print the domain and response for the user\n",
        "    print(f\"Domain: {response['Domain']}\")\n",
        "    print(f\"Response: {response['Response']}\")\n",
        "    print(f\"Sentiment: {response['Sentiment'][0]['label']} - {response['Sentiment'][0]['score']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "IcBVpcHzZSjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Load the notebook\n",
        "with open('Chatbot.ipynb', 'r', encoding='utf-8') as f:\n",
        "    notebook = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove 'metadata.widgets' if it's not well formatted or add 'state' to each widget\n",
        "if 'widgets' in notebook.metadata:\n",
        "    for widget in notebook.metadata['widgets']:\n",
        "        if 'state' not in widget:\n",
        "            widget['state'] = {}  # You can add an empty state or relevant information\n",
        "\n",
        "# Save the notebook with fixed metadata\n",
        "with open('Chatbot.ipynb', 'w', encoding='utf-8') as f:\n",
        "    nbformat.write(notebook, f)\n"
      ],
      "metadata": {
        "id": "0ltK6Mw0dADq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}