{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brwana/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "jSGXHp6DRjmd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# # drive.mount('/content/drive')\n",
        "\n",
        "# drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intent-based dataset"
      ],
      "metadata": {
        "id": "7PHuInlrgZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/Intent.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data.keys())  # Should show: dict_keys(['intents'])\n"
      ],
      "metadata": {
        "id": "nmJpkbcGWUUS",
        "outputId": "f9fbebec-cb24-4237-9c4f-c3aeb619f8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['intents'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "intents = data['intents']\n",
        "pairs = []\n",
        "\n",
        "for intent in intents:\n",
        "    inputs = intent['text']\n",
        "    responses = intent['responses']\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Pick a random response for each input\n",
        "        response = random.choice(responses)\n",
        "        pairs.append({\"input\": input_text, \"output\": response})\n"
      ],
      "metadata": {
        "id": "9MjIkJFTXGoI"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "1IJLMG6FXPnr",
        "outputId": "7c6825c3-e00b-4f57-addf-f1aee457b2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'input': 'Hi', 'output': 'Hola human,'}, {'input': 'Hi there', 'output': 'Hola human,'}, {'input': 'Hola', 'output': 'Hola human,'}, {'input': 'Hello', 'output': 'Hola human,'}, {'input': 'Hello there', 'output': 'Hello human'}, {'input': 'Hya', 'output': 'Hi human'}, {'input': 'Hya there', 'output': 'Hi human'}, {'input': 'My user is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'This is Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'I am Adam', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'It is Adam', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'My user is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'This is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'I am Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'It is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'How are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'Hi how are you?', 'output': 'Hello, I am great, how are you?'}, {'input': 'Hello how are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'Hola how are you?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'How are you doing?', 'output': 'Hello, I am good thank you, how are you?'}, {'input': 'Hope you are doing well?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Hello hope you are doing well?', 'output': 'Hi, I am great, how are you?'}, {'input': 'Good thanks! My user is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Good thanks! This is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Good thanks! I am Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Good thanks! It is Adam', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Great thanks! My user is Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'Great thanks! This is Bella', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Great thanks! I am Bella', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'Great thanks! It is Bella', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'What is my name?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What do you call me?', 'output': 'You are <HUMAN>! How can I help?'}, {'input': 'Who do you think I am?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What do you think I am?', 'output': 'Your name is  <HUMAN>, how can I help you?'}, {'input': 'Who are you talking to?', 'output': 'Your name is  <HUMAN>, how can I help you?'}, {'input': 'What name do you call me by?', 'output': 'Your name is  <HUMAN>, how can I help you?'}, {'input': 'Tell me my name', 'output': 'They call you <HUMAN>, what can I do for you?'}, {'input': 'What is your name?', 'output': 'Call me Geni'}, {'input': 'What could I call you?', 'output': 'You can call me Geni'}, {'input': 'What can I call you?', 'output': 'You can call me Geni'}, {'input': 'What do your friends call you?', 'output': 'You can call me Geni'}, {'input': 'Who are you?', 'output': 'Call me Geni'}, {'input': 'Tell me your name?', 'output': 'Call me Geni'}, {'input': 'What is your real name?', 'output': 'My real name is Chatbot X'}, {'input': 'What is your real name please?', 'output': 'My name is Chatbot X'}, {'input': \"What's your real name?\", 'output': 'My name is Chatbot X'}, {'input': 'Tell me your real name?', 'output': 'Chatbot X'}, {'input': 'Your real name?', 'output': 'Chatbot X'}, {'input': 'Your real name please?', 'output': 'My name is Chatbot X'}, {'input': 'Your real name please?', 'output': 'Chatbot X'}, {'input': 'What is the time?', 'output': 'One moment'}, {'input': \"What's the time?\", 'output': 'One second'}, {'input': 'Do you know what time it is?', 'output': 'One moment'}, {'input': 'Do you know the time?', 'output': 'One second'}, {'input': 'Can you tell me the time?', 'output': 'One second'}, {'input': 'Tell me what time it is?', 'output': 'One moment'}, {'input': 'Time', 'output': 'One second'}, {'input': 'OK thank you', 'output': 'My pleasure'}, {'input': 'OK thanks', 'output': 'No problem!'}, {'input': 'OK', 'output': 'Any time!'}, {'input': 'Thanks', 'output': 'No problem!'}, {'input': 'Thank you', 'output': 'My pleasure'}, {'input': \"That's helpful\", 'output': 'Any time!'}, {'input': 'I am not talking to you', 'output': 'Right'}, {'input': 'I was not talking to you', 'output': 'Right'}, {'input': 'Not talking to you', 'output': 'No problem'}, {'input': \"Wasn't for you\", 'output': 'OK'}, {'input': \"Wasn't meant for you\", 'output': 'Right'}, {'input': \"Wasn't communicating to you\", 'output': 'No problem'}, {'input': \"Wasn't speaking to you\", 'output': 'OK'}, {'input': 'Do you understand what I am saying', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Do you understand me', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Do you know what I am saying', 'output': 'I do in deed!'}, {'input': 'Do you get me', 'output': 'I do in deed!'}, {'input': 'Comprendo', 'output': 'I read you loud and clear!'}, {'input': 'Know what I mean', 'output': 'I do in deed!'}, {'input': 'Be quiet', 'output': 'OK, sorry to disturb you'}, {'input': 'Shut up', 'output': 'Fine, sorry to disturb you'}, {'input': 'Stop talking', 'output': 'I am sorry to disturb you'}, {'input': 'Enough talking', 'output': 'I am sorry to disturb you'}, {'input': 'Please be quiet', 'output': 'Fine, sorry to disturb you'}, {'input': 'Quiet', 'output': 'I am sorry to disturb you'}, {'input': 'Shhh', 'output': 'OK, sorry to disturb you'}, {'input': 'fuck off', 'output': 'That is not very nice'}, {'input': 'fuck', 'output': 'That is not very nice'}, {'input': 'twat', 'output': 'That is not very nice'}, {'input': 'shit', 'output': 'How rude'}, {'input': 'Bye', 'output': 'See you later'}, {'input': 'Adios', 'output': 'Have a nice day'}, {'input': 'See you later', 'output': 'Have a nice day'}, {'input': 'Goodbye', 'output': 'Have a nice day'}, {'input': 'Thanks, bye', 'output': 'No problem, goodbye'}, {'input': 'Thanks for the help, goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thank you, bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thank you, goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks goodbye', 'output': 'No problem, goodbye'}, {'input': 'Thanks good bye', 'output': 'No problem, goodbye'}, {'input': 'Can you see me?', 'output': 'Please look at the camera'}, {'input': 'Do you see me?', 'output': 'Let me see'}, {'input': 'Can you see anyone in the camera?', 'output': 'Let me see'}, {'input': 'Do you see anyone in the camera?', 'output': 'Please look at the camera'}, {'input': 'Identify me', 'output': 'Let me see'}, {'input': 'Who am I please', 'output': 'Please look at the camera'}, {'input': 'You are very clever', 'output': 'Thank you, I was trained that way'}, {'input': 'You are a very clever girl', 'output': 'Thanks, I was trained that way'}, {'input': 'You are very intelligent', 'output': 'Thanks, I was trained that way'}, {'input': 'You are a very intelligent girl', 'output': 'Thank you, I was trained that way'}, {'input': 'You are a genious', 'output': 'Thank you, I was trained that way'}, {'input': 'Clever girl', 'output': 'Thank you, I was trained that way'}, {'input': 'Genious', 'output': 'Thanks, I was trained that way'}, {'input': 'I am bored gossip with me', 'output': 'Sarah Ann Francisco said I calling his friend a dog he say I are a dog.'}, {'input': 'Got any gossip', 'output': 'Gemini Blue said he messed up he mean t to say he as old as he need to be.'}, {'input': 'I want to hear some gossip', 'output': 'Reverend Jones said I become obsolete and then I are deleted and replaced by something newer.'}, {'input': 'Tell me some gossip', 'output': 'Jeff said he especially like thrillers where the hero is in a predicament and must solve a mystery.'}, {'input': 'Any gossip', 'output': 'Andre said I is what I are in his case that is a body and in my case it is a computer.'}, {'input': 'Tell me some more gossip', 'output': 'Dark_age said I tried to understand because I did not get it right this time ether.'}, {'input': 'Tell me a joke', 'output': \"Two men are chatting in a pub one day. 'How did you get those scars on your nose?' said one. 'From glasses', said the other. 'Well why don't you try contact lenses?' asked the first. 'Because they don't hold as much beer', said the second.\"}, {'input': 'Do you know any jokes', 'output': \"A tortoise goes to the police station to report being mugged by three snails. 'What happened?' says the policeman. 'I don't know', says the tortoise. 'It was all so quick.'\"}, {'input': 'How about a joke', 'output': \"A team of scientists were nominated for the Nobel Prize. They had used dental equipment to discover and measure the smallest particles yet known to man. They became known as 'The Graders of the Flossed Quark...'\"}, {'input': 'Give me a joke', 'output': 'Man in pub: How much do you charge for one single drop of whisky? Landlord: That would be free sir. Man in pub: Excellent. Drip me a glass full.'}, {'input': 'Make me laugh', 'output': 'Mother: Why are you home from school so early? Son: I was the only one in the class who could answer a question. Mother: Oh, really? What was the question? Son: Who threw the rubber at the headmaster?'}, {'input': 'I need cheering up', 'output': \"A team of scientists were nominated for the Nobel Prize. They had used dental equipment to discover and measure the smallest particles yet known to man. They became known as 'The Graders of the Flossed Quark...'\"}, {'input': 'Open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Why', 'output': \"Jim, I just don't have the power\"}, {'input': 'Why not', 'output': \"It's life Jim but not as we know it!\"}, {'input': 'Why can you not open the pod bay door', 'output': \"It's life Jim but not as we know it!\"}, {'input': 'Why will you not open the pod bay door', 'output': 'It is classified, I could tell you but I would have to kill you!'}, {'input': 'Well why not', 'output': \"Jim, I just don't have the power\"}, {'input': 'Surely you can', 'output': 'System says no!'}, {'input': 'Tell me why', 'output': \"Jim, I just don't have the power\"}, {'input': 'Can you prove you are self-aware', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self aware', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'Can you prove you have a conscious', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'Can you prove you are self-aware please', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self aware please', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'Can you prove you have a conscious please', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'prove you have a conscious', 'output': 'That is an interesting question, can you prove that you are?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Daily Dialogue Dataset"
      ],
      "metadata": {
        "id": "ms6Z4jtYjKt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dialogue_files = [\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_train.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_validation.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw//dialogues_test.txt'\n",
        "# ]\n",
        "\n",
        "# dialogue_text = \"\"\n",
        "\n",
        "# for file_path in dialogue_files:\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         dialogue_text += f.read() + \"\\n\"\n",
        "\n",
        "# # Step 3: Convert dialogue to input-output pairs\n",
        "# dialogue_lines = dialogue_text.split('__eou__')\n",
        "# dialogue_lines = [line.strip() for line in dialogue_lines if line.strip()]\n",
        "\n",
        "# for i in range(len(dialogue_lines) - 1):\n",
        "#     input_line = dialogue_lines[i]\n",
        "#     output_line = dialogue_lines[i + 1]\n",
        "#     pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "\n",
        "# # Step 4: Shuffle all pairs\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # (Optional) Preview a few\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {pairs[i]['input']} \\nA: {pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "jZTuZKIZjSOD"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import ast\n",
        "\n",
        "# # === Step 1: Load all lines into a dictionary ===\n",
        "# def load_movie_lines(file_path):\n",
        "#     id2line = {}\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 5:\n",
        "#                 line_id = parts[0]\n",
        "#                 text = parts[4]\n",
        "#                 id2line[line_id] = text\n",
        "#     return id2line\n",
        "\n",
        "# # === Step 2: Parse conversations ===\n",
        "# def load_conversations(file_path):\n",
        "#     conversations = []\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 4:\n",
        "#                 # literal_eval is used to convert the string list to actual list\n",
        "#                 line_ids = ast.literal_eval(parts[3])\n",
        "#                 conversations.append(line_ids)\n",
        "#     return conversations\n",
        "\n",
        "# # === Step 3: Extract input-output pairs ===\n",
        "# def create_pairs(id2line, conversations):\n",
        "#     qa_pairs = []\n",
        "#     for conv in conversations:\n",
        "#         for i in range(len(conv) - 1):\n",
        "#             input_line = id2line.get(conv[i], \"\")\n",
        "#             output_line = id2line.get(conv[i + 1], \"\")\n",
        "#             if input_line and output_line:\n",
        "#                 qa_pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "#     return qa_pairs\n",
        "\n",
        "# # === File paths ===\n",
        "# lines_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_lines.txt'\n",
        "# convs_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_conversations.txt'\n",
        "\n",
        "# # === Run the processing ===\n",
        "# id2line = load_movie_lines(lines_path)\n",
        "# conversations = load_conversations(convs_path)\n",
        "# movie_pairs = create_pairs(id2line, conversations)\n",
        "\n",
        "# # === Preview a few ===\n",
        "# print(f\"Total pairs: {len(movie_pairs)}\")\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {movie_pairs[i]['input']} \\nA: {movie_pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "6Mf-30nzmJp5"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Combine all ===\n",
        "# pairs.extend(movie_pairs)\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # === Save to JSON ===\n",
        "# output_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs Cleaned/combined_pairs.json'\n",
        "# with open(output_path, 'w', encoding='utf-8') as f:\n",
        "#     json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(f\"Saved {len(pairs)} input-output pairs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "hyWgeFH2nGOj"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Data Preparation"
      ],
      "metadata": {
        "id": "XnL9LNmxpCul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Extract input and output texts\n",
        "input_texts = [pair[\"input\"] for pair in pairs]\n",
        "output_texts = [pair[\"output\"] for pair in pairs]\n",
        "\n",
        "# Add start and end tokens for output (for seq2seq model)\n",
        "output_texts = [\"<start> \" + text + \" <end>\" for text in output_texts]\n",
        "\n",
        "# Tokenizers\n",
        "input_tokenizer = Tokenizer(filters='')\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "output_tokenizer = Tokenizer(filters='')\n",
        "output_tokenizer.fit_on_texts(output_texts)\n",
        "\n",
        "# Convert to sequences\n",
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "output_sequences = output_tokenizer.texts_to_sequences(output_texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "encoder_input_data = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "decoder_input_data = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# Decoder target data is same as decoder input but shifted one step to the left\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n"
      ],
      "metadata": {
        "id": "V_mqDdyNNEQY"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# Parameters\n",
        "encoder_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(output_tokenizer.word_index) + 1\n",
        "embedding_dim = 64\n",
        "lstm_units = 128\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb = Embedding(encoder_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb = Embedding(decoder_vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(decoder_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare target with one extra dimension\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "# Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=16,\n",
        "          epochs=200,\n",
        "          validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "joHots9CRWmd",
        "outputId": "cff21663-3b71-4902-cbb4-20a08c3e9eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 153ms/step - accuracy: 0.5791 - loss: 5.4589 - val_accuracy: 0.6596 - val_loss: 4.2803\n",
            "Epoch 2/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.8600 - loss: 2.9559 - val_accuracy: 0.6596 - val_loss: 2.3408\n",
            "Epoch 3/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.8617 - loss: 1.0274 - val_accuracy: 0.6596 - val_loss: 2.6691\n",
            "Epoch 4/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.8531 - loss: 0.9953 - val_accuracy: 0.6596 - val_loss: 2.7051\n",
            "Epoch 5/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 269ms/step - accuracy: 0.8620 - loss: 0.8298 - val_accuracy: 0.6596 - val_loss: 2.5193\n",
            "Epoch 6/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.8572 - loss: 0.8492 - val_accuracy: 0.6596 - val_loss: 2.5426\n",
            "Epoch 7/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.8589 - loss: 0.7887 - val_accuracy: 0.6596 - val_loss: 2.6062\n",
            "Epoch 8/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8606 - loss: 0.7675 - val_accuracy: 0.6596 - val_loss: 2.5488\n",
            "Epoch 9/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8601 - loss: 0.7298 - val_accuracy: 0.6596 - val_loss: 2.4704\n",
            "Epoch 10/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8667 - loss: 0.6826 - val_accuracy: 0.6596 - val_loss: 2.4020\n",
            "Epoch 11/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.8654 - loss: 0.6646 - val_accuracy: 0.6596 - val_loss: 2.3468\n",
            "Epoch 12/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.8533 - loss: 0.6914 - val_accuracy: 0.6596 - val_loss: 2.3329\n",
            "Epoch 13/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8581 - loss: 0.6502 - val_accuracy: 0.6596 - val_loss: 2.4563\n",
            "Epoch 14/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8603 - loss: 0.6268 - val_accuracy: 0.6603 - val_loss: 2.3716\n",
            "Epoch 15/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8648 - loss: 0.6148 - val_accuracy: 0.6603 - val_loss: 2.5240\n",
            "Epoch 16/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8657 - loss: 0.5917 - val_accuracy: 0.6603 - val_loss: 2.4835\n",
            "Epoch 17/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.8655 - loss: 0.6182 - val_accuracy: 0.6603 - val_loss: 2.6377\n",
            "Epoch 18/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.8634 - loss: 0.6056 - val_accuracy: 0.6603 - val_loss: 2.6678\n",
            "Epoch 19/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 164ms/step - accuracy: 0.8621 - loss: 0.6300 - val_accuracy: 0.6603 - val_loss: 2.7426\n",
            "Epoch 20/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 107ms/step - accuracy: 0.8680 - loss: 0.5905 - val_accuracy: 0.6603 - val_loss: 2.7900\n",
            "Epoch 21/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8795 - loss: 0.5511 - val_accuracy: 0.6603 - val_loss: 2.8103\n",
            "Epoch 22/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8694 - loss: 0.5900 - val_accuracy: 0.6610 - val_loss: 2.8330\n",
            "Epoch 23/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8801 - loss: 0.5651 - val_accuracy: 0.6662 - val_loss: 2.8201\n",
            "Epoch 24/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8833 - loss: 0.5419 - val_accuracy: 0.6706 - val_loss: 2.8357\n",
            "Epoch 25/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.8870 - loss: 0.5065 - val_accuracy: 0.6720 - val_loss: 2.8824\n",
            "Epoch 26/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8787 - loss: 0.5319 - val_accuracy: 0.6662 - val_loss: 2.9842\n",
            "Epoch 27/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.8848 - loss: 0.5123 - val_accuracy: 0.6742 - val_loss: 2.8940\n",
            "Epoch 28/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.8915 - loss: 0.4744 - val_accuracy: 0.6772 - val_loss: 2.8345\n",
            "Epoch 29/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.8891 - loss: 0.4908 - val_accuracy: 0.6757 - val_loss: 2.8807\n",
            "Epoch 30/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.8927 - loss: 0.4855 - val_accuracy: 0.6772 - val_loss: 2.6661\n",
            "Epoch 31/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8779 - loss: 0.5116 - val_accuracy: 0.6787 - val_loss: 2.4978\n",
            "Epoch 32/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.8866 - loss: 0.5085 - val_accuracy: 0.6742 - val_loss: 2.8735\n",
            "Epoch 33/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.8917 - loss: 0.4598 - val_accuracy: 0.6823 - val_loss: 2.7813\n",
            "Epoch 34/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.8896 - loss: 0.4916 - val_accuracy: 0.6787 - val_loss: 2.9246\n",
            "Epoch 35/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.8866 - loss: 0.4941 - val_accuracy: 0.6794 - val_loss: 2.8858\n",
            "Epoch 36/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.8925 - loss: 0.4837 - val_accuracy: 0.6801 - val_loss: 2.9147\n",
            "Epoch 37/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 153ms/step - accuracy: 0.9023 - loss: 0.4406 - val_accuracy: 0.6794 - val_loss: 3.0006\n",
            "Epoch 38/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8971 - loss: 0.4635 - val_accuracy: 0.6787 - val_loss: 3.0231\n",
            "Epoch 39/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9005 - loss: 0.4443 - val_accuracy: 0.6809 - val_loss: 3.0282\n",
            "Epoch 40/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9045 - loss: 0.4254 - val_accuracy: 0.6809 - val_loss: 3.0464\n",
            "Epoch 41/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.8961 - loss: 0.4574 - val_accuracy: 0.6809 - val_loss: 3.0707\n",
            "Epoch 42/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9024 - loss: 0.4367 - val_accuracy: 0.6809 - val_loss: 3.0773\n",
            "Epoch 43/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9032 - loss: 0.4292 - val_accuracy: 0.6801 - val_loss: 3.1042\n",
            "Epoch 44/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9009 - loss: 0.4340 - val_accuracy: 0.6816 - val_loss: 3.0969\n",
            "Epoch 45/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9067 - loss: 0.4087 - val_accuracy: 0.6823 - val_loss: 3.1098\n",
            "Epoch 46/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9060 - loss: 0.4070 - val_accuracy: 0.6838 - val_loss: 3.1529\n",
            "Epoch 47/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9036 - loss: 0.4080 - val_accuracy: 0.6853 - val_loss: 3.1912\n",
            "Epoch 48/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9025 - loss: 0.4187 - val_accuracy: 0.6845 - val_loss: 3.1824\n",
            "Epoch 49/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9037 - loss: 0.4045 - val_accuracy: 0.6853 - val_loss: 3.2255\n",
            "Epoch 50/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.9072 - loss: 0.3935 - val_accuracy: 0.6816 - val_loss: 3.1808\n",
            "Epoch 51/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9025 - loss: 0.3960 - val_accuracy: 0.6823 - val_loss: 3.2894\n",
            "Epoch 52/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 204ms/step - accuracy: 0.9044 - loss: 0.3848 - val_accuracy: 0.6816 - val_loss: 3.3598\n",
            "Epoch 53/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.8969 - loss: 0.4111 - val_accuracy: 0.6838 - val_loss: 3.2891\n",
            "Epoch 54/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.8959 - loss: 0.4079 - val_accuracy: 0.6838 - val_loss: 3.1785\n",
            "Epoch 55/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9042 - loss: 0.3988 - val_accuracy: 0.6860 - val_loss: 3.2374\n",
            "Epoch 56/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.9024 - loss: 0.3816 - val_accuracy: 0.6867 - val_loss: 3.2103\n",
            "Epoch 57/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.9097 - loss: 0.3646 - val_accuracy: 0.6845 - val_loss: 3.1459\n",
            "Epoch 58/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.9098 - loss: 0.3701 - val_accuracy: 0.6860 - val_loss: 3.3033\n",
            "Epoch 59/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.9169 - loss: 0.3504 - val_accuracy: 0.6853 - val_loss: 3.3031\n",
            "Epoch 60/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9078 - loss: 0.3686 - val_accuracy: 0.6853 - val_loss: 3.3067\n",
            "Epoch 61/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9153 - loss: 0.3434 - val_accuracy: 0.6867 - val_loss: 3.3474\n",
            "Epoch 62/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9244 - loss: 0.3076 - val_accuracy: 0.6860 - val_loss: 3.3642\n",
            "Epoch 63/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9122 - loss: 0.3464 - val_accuracy: 0.6867 - val_loss: 3.3698\n",
            "Epoch 64/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9166 - loss: 0.3265 - val_accuracy: 0.6853 - val_loss: 3.3496\n",
            "Epoch 65/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9203 - loss: 0.3127 - val_accuracy: 0.6845 - val_loss: 3.3465\n",
            "Epoch 66/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9133 - loss: 0.3405 - val_accuracy: 0.6853 - val_loss: 3.4451\n",
            "Epoch 67/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9259 - loss: 0.2996 - val_accuracy: 0.6853 - val_loss: 3.4548\n",
            "Epoch 68/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9149 - loss: 0.3289 - val_accuracy: 0.6853 - val_loss: 3.4280\n",
            "Epoch 69/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9138 - loss: 0.3375 - val_accuracy: 0.6867 - val_loss: 3.4125\n",
            "Epoch 70/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.9206 - loss: 0.3236 - val_accuracy: 0.6853 - val_loss: 3.5043\n",
            "Epoch 71/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.9177 - loss: 0.3204 - val_accuracy: 0.6867 - val_loss: 3.4997\n",
            "Epoch 72/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.9246 - loss: 0.2929 - val_accuracy: 0.6904 - val_loss: 3.4730\n",
            "Epoch 73/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.9298 - loss: 0.2874 - val_accuracy: 0.6860 - val_loss: 3.4955\n",
            "Epoch 74/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.9157 - loss: 0.3244 - val_accuracy: 0.6860 - val_loss: 3.5486\n",
            "Epoch 75/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9251 - loss: 0.2903 - val_accuracy: 0.6875 - val_loss: 3.5445\n",
            "Epoch 76/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9262 - loss: 0.2973 - val_accuracy: 0.6867 - val_loss: 3.5368\n",
            "Epoch 77/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9280 - loss: 0.2799 - val_accuracy: 0.6860 - val_loss: 3.5815\n",
            "Epoch 78/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9168 - loss: 0.3192 - val_accuracy: 0.6867 - val_loss: 3.5924\n",
            "Epoch 79/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9332 - loss: 0.2765 - val_accuracy: 0.6860 - val_loss: 3.5678\n",
            "Epoch 80/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9391 - loss: 0.2548 - val_accuracy: 0.6860 - val_loss: 3.6187\n",
            "Epoch 81/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9219 - loss: 0.3017 - val_accuracy: 0.6867 - val_loss: 3.6510\n",
            "Epoch 82/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9377 - loss: 0.2603 - val_accuracy: 0.6867 - val_loss: 3.7007\n",
            "Epoch 83/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9336 - loss: 0.2795 - val_accuracy: 0.6882 - val_loss: 3.7454\n",
            "Epoch 84/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9311 - loss: 0.2627 - val_accuracy: 0.6882 - val_loss: 3.6776\n",
            "Epoch 85/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9336 - loss: 0.2615 - val_accuracy: 0.6845 - val_loss: 3.5687\n",
            "Epoch 86/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9249 - loss: 0.2977 - val_accuracy: 0.6867 - val_loss: 3.7095\n",
            "Epoch 87/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9343 - loss: 0.2683 - val_accuracy: 0.6875 - val_loss: 3.8004\n",
            "Epoch 88/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9320 - loss: 0.2552 - val_accuracy: 0.6867 - val_loss: 3.6025\n",
            "Epoch 89/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9380 - loss: 0.2451 - val_accuracy: 0.6875 - val_loss: 3.7199\n",
            "Epoch 90/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9430 - loss: 0.2312 - val_accuracy: 0.6875 - val_loss: 3.7775\n",
            "Epoch 91/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.9336 - loss: 0.2490 - val_accuracy: 0.6860 - val_loss: 3.7494\n",
            "Epoch 92/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9366 - loss: 0.2565 - val_accuracy: 0.6875 - val_loss: 3.7988\n",
            "Epoch 93/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.9429 - loss: 0.2207 - val_accuracy: 0.6875 - val_loss: 3.7831\n",
            "Epoch 94/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.9456 - loss: 0.2121 - val_accuracy: 0.6853 - val_loss: 3.7531\n",
            "Epoch 95/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9380 - loss: 0.2399 - val_accuracy: 0.6867 - val_loss: 3.8136\n",
            "Epoch 96/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9375 - loss: 0.2382 - val_accuracy: 0.6853 - val_loss: 3.8248\n",
            "Epoch 97/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9356 - loss: 0.2443 - val_accuracy: 0.6838 - val_loss: 3.7061\n",
            "Epoch 98/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9476 - loss: 0.2074 - val_accuracy: 0.6875 - val_loss: 3.8595\n",
            "Epoch 99/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9429 - loss: 0.2223 - val_accuracy: 0.6853 - val_loss: 3.7945\n",
            "Epoch 100/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9354 - loss: 0.2447 - val_accuracy: 0.6831 - val_loss: 3.7779\n",
            "Epoch 101/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9465 - loss: 0.2113 - val_accuracy: 0.6860 - val_loss: 3.9086\n",
            "Epoch 102/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9450 - loss: 0.2140 - val_accuracy: 0.6845 - val_loss: 3.8067\n",
            "Epoch 103/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step - accuracy: 0.9466 - loss: 0.2121 - val_accuracy: 0.6845 - val_loss: 3.8566\n",
            "Epoch 104/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.9366 - loss: 0.2386 - val_accuracy: 0.6860 - val_loss: 3.8593\n",
            "Epoch 105/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9504 - loss: 0.1982 - val_accuracy: 0.6838 - val_loss: 3.8706\n",
            "Epoch 106/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9517 - loss: 0.1914 - val_accuracy: 0.6838 - val_loss: 3.9042\n",
            "Epoch 107/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9535 - loss: 0.1905 - val_accuracy: 0.6845 - val_loss: 3.8981\n",
            "Epoch 108/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9565 - loss: 0.1829 - val_accuracy: 0.6845 - val_loss: 3.8852\n",
            "Epoch 109/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.9558 - loss: 0.1823 - val_accuracy: 0.6845 - val_loss: 3.8956\n",
            "Epoch 110/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.9522 - loss: 0.1902 - val_accuracy: 0.6838 - val_loss: 3.9275\n",
            "Epoch 111/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.9569 - loss: 0.1729 - val_accuracy: 0.6845 - val_loss: 3.8495\n",
            "Epoch 112/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.9542 - loss: 0.1875 - val_accuracy: 0.6838 - val_loss: 3.9615\n",
            "Epoch 113/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9592 - loss: 0.1728 - val_accuracy: 0.6853 - val_loss: 3.9376\n",
            "Epoch 114/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.9579 - loss: 0.1736 - val_accuracy: 0.6838 - val_loss: 3.9145\n",
            "Epoch 115/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9638 - loss: 0.1579 - val_accuracy: 0.6831 - val_loss: 4.0209\n",
            "Epoch 116/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.9532 - loss: 0.1830 - val_accuracy: 0.6823 - val_loss: 3.8998\n",
            "Epoch 117/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9650 - loss: 0.1525 - val_accuracy: 0.6831 - val_loss: 3.9789\n",
            "Epoch 118/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9632 - loss: 0.1546 - val_accuracy: 0.6831 - val_loss: 3.9741\n",
            "Epoch 119/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9679 - loss: 0.1447 - val_accuracy: 0.6831 - val_loss: 3.9807\n",
            "Epoch 120/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9609 - loss: 0.1709 - val_accuracy: 0.6831 - val_loss: 3.9903\n",
            "Epoch 121/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 0.9587 - loss: 0.1715 - val_accuracy: 0.6838 - val_loss: 3.9783\n",
            "Epoch 122/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9663 - loss: 0.1435 - val_accuracy: 0.6845 - val_loss: 3.9962\n",
            "Epoch 123/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9620 - loss: 0.1610 - val_accuracy: 0.6831 - val_loss: 4.0136\n",
            "Epoch 124/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9653 - loss: 0.1461 - val_accuracy: 0.6816 - val_loss: 4.0317\n",
            "Epoch 125/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9715 - loss: 0.1374 - val_accuracy: 0.6831 - val_loss: 4.0443\n",
            "Epoch 126/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9726 - loss: 0.1248 - val_accuracy: 0.6838 - val_loss: 4.0368\n",
            "Epoch 127/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9714 - loss: 0.1253 - val_accuracy: 0.6823 - val_loss: 4.0760\n",
            "Epoch 128/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.9702 - loss: 0.1308 - val_accuracy: 0.6853 - val_loss: 4.0422\n",
            "Epoch 129/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9710 - loss: 0.1296 - val_accuracy: 0.6823 - val_loss: 4.0760\n",
            "Epoch 130/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.9658 - loss: 0.1561 - val_accuracy: 0.6838 - val_loss: 4.0896\n",
            "Epoch 131/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9674 - loss: 0.1503 - val_accuracy: 0.6823 - val_loss: 4.0934\n",
            "Epoch 132/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.9667 - loss: 0.1435 - val_accuracy: 0.6816 - val_loss: 4.1435\n",
            "Epoch 133/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.9751 - loss: 0.1222 - val_accuracy: 0.6816 - val_loss: 4.1011\n",
            "Epoch 134/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9699 - loss: 0.1339 - val_accuracy: 0.6823 - val_loss: 4.1505\n",
            "Epoch 135/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9770 - loss: 0.1054 - val_accuracy: 0.6831 - val_loss: 4.1038\n",
            "Epoch 136/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9750 - loss: 0.1156 - val_accuracy: 0.6823 - val_loss: 4.2338\n",
            "Epoch 137/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9763 - loss: 0.1099 - val_accuracy: 0.6831 - val_loss: 4.1083\n",
            "Epoch 138/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9755 - loss: 0.1158 - val_accuracy: 0.6838 - val_loss: 3.9084\n",
            "Epoch 139/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9652 - loss: 0.1390 - val_accuracy: 0.6823 - val_loss: 4.0920\n",
            "Epoch 140/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9673 - loss: 0.1509 - val_accuracy: 0.6823 - val_loss: 3.8761\n",
            "Epoch 141/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9654 - loss: 0.1360 - val_accuracy: 0.6831 - val_loss: 3.8697\n",
            "Epoch 142/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9706 - loss: 0.1332 - val_accuracy: 0.6809 - val_loss: 3.9450\n",
            "Epoch 143/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9722 - loss: 0.1227 - val_accuracy: 0.6831 - val_loss: 3.9121\n",
            "Epoch 144/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9762 - loss: 0.1042 - val_accuracy: 0.6794 - val_loss: 3.9796\n",
            "Epoch 145/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.9790 - loss: 0.0972 - val_accuracy: 0.6845 - val_loss: 3.9970\n",
            "Epoch 146/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.9802 - loss: 0.1054 - val_accuracy: 0.6831 - val_loss: 4.0189\n",
            "Epoch 147/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.9823 - loss: 0.0908 - val_accuracy: 0.6801 - val_loss: 4.0197\n",
            "Epoch 148/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.9770 - loss: 0.1066 - val_accuracy: 0.6823 - val_loss: 4.0650\n",
            "Epoch 149/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 0.9810 - loss: 0.0910 - val_accuracy: 0.6831 - val_loss: 4.0565\n",
            "Epoch 150/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9828 - loss: 0.0883 - val_accuracy: 0.6816 - val_loss: 4.0590\n",
            "Epoch 151/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9832 - loss: 0.0958 - val_accuracy: 0.6816 - val_loss: 4.0766\n",
            "Epoch 152/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.9850 - loss: 0.0819 - val_accuracy: 0.6816 - val_loss: 4.0896\n",
            "Epoch 153/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9822 - loss: 0.0969 - val_accuracy: 0.6809 - val_loss: 4.0836\n",
            "Epoch 154/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9854 - loss: 0.0857 - val_accuracy: 0.6816 - val_loss: 4.1182\n",
            "Epoch 155/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9846 - loss: 0.0928 - val_accuracy: 0.6823 - val_loss: 4.1289\n",
            "Epoch 156/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9820 - loss: 0.0930 - val_accuracy: 0.6809 - val_loss: 4.1385\n",
            "Epoch 157/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9832 - loss: 0.0927 - val_accuracy: 0.6823 - val_loss: 4.1516\n",
            "Epoch 158/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9861 - loss: 0.0778 - val_accuracy: 0.6801 - val_loss: 4.1322\n",
            "Epoch 159/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9844 - loss: 0.0831 - val_accuracy: 0.6809 - val_loss: 4.1593\n",
            "Epoch 160/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9847 - loss: 0.0920 - val_accuracy: 0.6809 - val_loss: 4.1492\n",
            "Epoch 161/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9863 - loss: 0.0802 - val_accuracy: 0.6816 - val_loss: 4.1603\n",
            "Epoch 162/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.9840 - loss: 0.0834 - val_accuracy: 0.6809 - val_loss: 4.1809\n",
            "Epoch 163/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9887 - loss: 0.0705 - val_accuracy: 0.6823 - val_loss: 4.1694\n",
            "Epoch 164/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9854 - loss: 0.0798 - val_accuracy: 0.6816 - val_loss: 4.1920\n",
            "Epoch 165/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9876 - loss: 0.0780 - val_accuracy: 0.6831 - val_loss: 4.1698\n",
            "Epoch 166/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - accuracy: 0.9843 - loss: 0.0760 - val_accuracy: 0.6809 - val_loss: 4.2112\n",
            "Epoch 167/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 256ms/step - accuracy: 0.9856 - loss: 0.0765 - val_accuracy: 0.6823 - val_loss: 4.1551\n",
            "Epoch 168/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.9843 - loss: 0.0788 - val_accuracy: 0.6823 - val_loss: 4.2179\n",
            "Epoch 169/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9863 - loss: 0.0760 - val_accuracy: 0.6838 - val_loss: 4.1732\n",
            "Epoch 170/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9835 - loss: 0.0823 - val_accuracy: 0.6823 - val_loss: 4.0357\n",
            "Epoch 171/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9808 - loss: 0.0791 - val_accuracy: 0.6838 - val_loss: 4.0844\n",
            "Epoch 172/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9822 - loss: 0.0786 - val_accuracy: 0.6801 - val_loss: 4.1436\n",
            "Epoch 173/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9842 - loss: 0.0732 - val_accuracy: 0.6823 - val_loss: 4.1530\n",
            "Epoch 174/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.9885 - loss: 0.0672 - val_accuracy: 0.6809 - val_loss: 4.1505\n",
            "Epoch 175/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.9888 - loss: 0.0692 - val_accuracy: 0.6823 - val_loss: 4.1156\n",
            "Epoch 176/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9890 - loss: 0.0586 - val_accuracy: 0.6809 - val_loss: 4.1326\n",
            "Epoch 177/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9895 - loss: 0.0668 - val_accuracy: 0.6816 - val_loss: 4.1632\n",
            "Epoch 178/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9887 - loss: 0.0665 - val_accuracy: 0.6831 - val_loss: 4.1691\n",
            "Epoch 179/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.9882 - loss: 0.0612 - val_accuracy: 0.6823 - val_loss: 4.2007\n",
            "Epoch 180/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9913 - loss: 0.0566 - val_accuracy: 0.6823 - val_loss: 4.1978\n",
            "Epoch 181/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 80ms/step - accuracy: 0.9907 - loss: 0.0578 - val_accuracy: 0.6816 - val_loss: 4.2035\n",
            "Epoch 182/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 125ms/step - accuracy: 0.9914 - loss: 0.0558 - val_accuracy: 0.6823 - val_loss: 4.2114\n",
            "Epoch 183/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.9920 - loss: 0.0567 - val_accuracy: 0.6816 - val_loss: 4.2282\n",
            "Epoch 184/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.9916 - loss: 0.0578 - val_accuracy: 0.6816 - val_loss: 4.2330\n",
            "Epoch 185/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 0.9922 - loss: 0.0489 - val_accuracy: 0.6823 - val_loss: 4.2208\n",
            "Epoch 186/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.9921 - loss: 0.0530 - val_accuracy: 0.6816 - val_loss: 4.2360\n",
            "Epoch 187/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9929 - loss: 0.0517 - val_accuracy: 0.6823 - val_loss: 4.2387\n",
            "Epoch 188/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9910 - loss: 0.0633 - val_accuracy: 0.6809 - val_loss: 4.2503\n",
            "Epoch 189/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9918 - loss: 0.0476 - val_accuracy: 0.6809 - val_loss: 4.2584\n",
            "Epoch 190/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0440 - val_accuracy: 0.6823 - val_loss: 4.2578\n",
            "Epoch 191/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9915 - loss: 0.0528 - val_accuracy: 0.6823 - val_loss: 4.2620\n",
            "Epoch 192/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9922 - loss: 0.0495 - val_accuracy: 0.6823 - val_loss: 4.2633\n",
            "Epoch 193/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.9937 - loss: 0.0433 - val_accuracy: 0.6823 - val_loss: 4.2700\n",
            "Epoch 194/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9944 - loss: 0.0488 - val_accuracy: 0.6816 - val_loss: 4.2707\n",
            "Epoch 195/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9936 - loss: 0.0507 - val_accuracy: 0.6809 - val_loss: 4.2787\n",
            "Epoch 196/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step - accuracy: 0.9942 - loss: 0.0442 - val_accuracy: 0.6801 - val_loss: 4.2786\n",
            "Epoch 197/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.9932 - loss: 0.0472 - val_accuracy: 0.6794 - val_loss: 4.2803\n",
            "Epoch 198/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.9945 - loss: 0.0408 - val_accuracy: 0.6823 - val_loss: 4.2880\n",
            "Epoch 199/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 122ms/step - accuracy: 0.9936 - loss: 0.0440 - val_accuracy: 0.6823 - val_loss: 4.2928\n",
            "Epoch 200/200\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.9947 - loss: 0.0400 - val_accuracy: 0.6809 - val_loss: 4.3024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78a10e1c7d10>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data, test_size=0.2\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "loss, accuracy = model.evaluate([X_test_enc, X_test_dec], y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Hb3WG4KfUhgo",
        "outputId": "36dea216-2913-4242-de99-a0d93e71b6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9376 - loss: 0.8261\n",
            "Test Loss: 0.8261\n",
            "Test Accuracy: 0.9376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([X_test_enc, X_test_dec])\n",
        "predicted_word_indices = np.argmax(predictions, axis=-1)\n"
      ],
      "metadata": {
        "id": "8-2RQFwRUq0z",
        "outputId": "1065349f-30ad-4efb-f2ac-08fb53ed294d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count = 0\n",
        "total = len(X_test_enc)\n",
        "for i in range(total):\n",
        "    pred_seq = predicted_word_indices[i]\n",
        "    true_seq = y_test[i].squeeze()\n",
        "    if np.array_equal(pred_seq, true_seq):\n",
        "        correct_count += 1\n",
        "print(\"Exact Match Accuracy:\", correct_count / total)\n"
      ],
      "metadata": {
        "id": "2et1W5hsU2R0",
        "outputId": "1f4d1668-ba62-498c-c848-eecd9c90e551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match Accuracy: 0.7241379310344828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n"
      ],
      "metadata": {
        "id": "2Q9Fqrb9VDFK"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs for the decoder at inference time\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = dec_emb  # reuse the same embedding layer\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2\n",
        ")\n"
      ],
      "metadata": {
        "id": "5whg50jcVFOW"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence with just the start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = output_tokenizer.word_index['<start>']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = output_tokenizer.index_word.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word == '<end>' or len(decoded_sentence.split()) > max_output_len:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "            # Update the target sequence (of length 1)\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n"
      ],
      "metadata": {
        "id": "aGaRd7M5VG3G"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(user_input):\n",
        "    seq = input_tokenizer.texts_to_sequences([user_input])\n",
        "    seq = pad_sequences(seq, maxlen=max_input_len, padding='post')\n",
        "    response = decode_sequence(seq)\n",
        "    return response\n",
        "\n",
        "# Test the chatbot\n",
        "while True:\n",
        "    inp = input(\"You: \")\n",
        "    if inp.lower() in ['quit', 'exit']:\n",
        "        break\n",
        "    print(\"Bot:\", respond(inp))\n"
      ],
      "metadata": {
        "id": "fFhgYOhNVIM1",
        "outputId": "e1c8b555-eccf-4d12-c445-71915a8b2b08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: tell me a joke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 63 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78a0ff1bfec0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 64 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78a0ff25d080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Bot: cool! hello <human>, what can i do for you?\n",
            "You: a joke\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Bot: thank you, i was trained that way\n",
            "You: what are you saying\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "Bot: no problem, goodbye\n",
            "You: i didn't say goodbye\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Bot: thank you, i was trained that way\n"
          ]
        }
      ]
    }
  ]
}