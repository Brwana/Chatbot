{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brwana/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jSGXHp6DRjmd",
        "outputId": "392349fe-8f91-48ec-85e7-f0403b66e852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intent-based dataset"
      ],
      "metadata": {
        "id": "7PHuInlrgZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/Intent.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data.keys())  # Should show: dict_keys(['intents'])\n"
      ],
      "metadata": {
        "id": "nmJpkbcGWUUS",
        "outputId": "22f01ad1-1ac1-4fea-e76e-c629bead7776",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['intents'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "intents = data['intents']\n",
        "pairs = []\n",
        "\n",
        "for intent in intents:\n",
        "    inputs = intent['text']\n",
        "    responses = intent['responses']\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Pick a random response for each input\n",
        "        response = random.choice(responses)\n",
        "        pairs.append({\"input\": input_text, \"output\": response})\n"
      ],
      "metadata": {
        "id": "9MjIkJFTXGoI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "1IJLMG6FXPnr",
        "outputId": "609ebd3d-2c44-49d0-9412-765704b3338f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'input': 'Hi', 'output': 'Hello human'}, {'input': 'Hi there', 'output': 'Hello human'}, {'input': 'Hola', 'output': 'Hola human,'}, {'input': 'Hello', 'output': 'Hi human'}, {'input': 'Hello there', 'output': 'Hi human'}, {'input': 'Hya', 'output': 'Hola human,'}, {'input': 'Hya there', 'output': 'Hi human'}, {'input': 'My user is Adam', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'This is Adam', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'I am Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'It is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'My user is Bella', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'This is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'I am Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'It is Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'How are you?', 'output': 'Hello, I am good thank you, how are you?'}, {'input': 'Hi how are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'Hello how are you?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Hola how are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'How are you doing?', 'output': 'Hi, good thank you, how are you?'}, {'input': 'Hope you are doing well?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Hello hope you are doing well?', 'output': 'Hi, good thank you, how are you?'}, {'input': 'Good thanks! My user is Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'Good thanks! This is Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'Good thanks! I am Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'Good thanks! It is Adam', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Great thanks! My user is Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'Great thanks! This is Bella', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Great thanks! I am Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Great thanks! It is Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'What is my name?', 'output': 'You are <HUMAN>! How can I help?'}, {'input': 'What do you call me?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'Who do you think I am?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What do you think I am?', 'output': 'Your name is  <HUMAN>, how can I help you?'}, {'input': 'Who are you talking to?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What name do you call me by?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'Tell me my name', 'output': 'They call you <HUMAN>, what can I do for you?'}, {'input': 'What is your name?', 'output': 'Call me Geni'}, {'input': 'What could I call you?', 'output': 'Call me Geni'}, {'input': 'What can I call you?', 'output': 'Call me Geni'}, {'input': 'What do your friends call you?', 'output': 'You may call me Geni'}, {'input': 'Who are you?', 'output': 'Call me Geni'}, {'input': 'Tell me your name?', 'output': 'Call me Geni'}, {'input': 'What is your real name?', 'output': 'My name is Chatbot X'}, {'input': 'What is your real name please?', 'output': 'My real name is Chatbot X'}, {'input': \"What's your real name?\", 'output': 'My real name is Chatbot X'}, {'input': 'Tell me your real name?', 'output': 'My real name is Chatbot X'}, {'input': 'Your real name?', 'output': 'My real name is Chatbot X'}, {'input': 'Your real name please?', 'output': 'My real name is Chatbot X'}, {'input': 'Your real name please?', 'output': 'My name is Chatbot X'}, {'input': 'What is the time?', 'output': 'One sec'}, {'input': \"What's the time?\", 'output': 'One moment'}, {'input': 'Do you know what time it is?', 'output': 'One moment'}, {'input': 'Do you know the time?', 'output': 'One moment'}, {'input': 'Can you tell me the time?', 'output': 'One second'}, {'input': 'Tell me what time it is?', 'output': 'One second'}, {'input': 'Time', 'output': 'One sec'}, {'input': 'OK thank you', 'output': 'My pleasure'}, {'input': 'OK thanks', 'output': 'Any time!'}, {'input': 'OK', 'output': 'Happy to help!'}, {'input': 'Thanks', 'output': 'Happy to help!'}, {'input': 'Thank you', 'output': 'My pleasure'}, {'input': \"That's helpful\", 'output': 'No problem!'}, {'input': 'I am not talking to you', 'output': 'No problem'}, {'input': 'I was not talking to you', 'output': 'Right'}, {'input': 'Not talking to you', 'output': 'No problem'}, {'input': \"Wasn't for you\", 'output': 'No problem'}, {'input': \"Wasn't meant for you\", 'output': 'No problem'}, {'input': \"Wasn't communicating to you\", 'output': 'OK'}, {'input': \"Wasn't speaking to you\", 'output': 'Right'}, {'input': 'Do you understand what I am saying', 'output': 'I do in deed!'}, {'input': 'Do you understand me', 'output': 'I read you loud and clear!'}, {'input': 'Do you know what I am saying', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Do you get me', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Comprendo', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Know what I mean', 'output': 'I read you loud and clear!'}, {'input': 'Be quiet', 'output': 'Fine, sorry to disturb you'}, {'input': 'Shut up', 'output': 'I am sorry to disturb you'}, {'input': 'Stop talking', 'output': 'I am sorry to disturb you'}, {'input': 'Enough talking', 'output': 'I am sorry to disturb you'}, {'input': 'Please be quiet', 'output': 'Fine, sorry to disturb you'}, {'input': 'Quiet', 'output': 'I am sorry to disturb you'}, {'input': 'Shhh', 'output': 'Fine, sorry to disturb you'}, {'input': 'fuck off', 'output': 'How rude'}, {'input': 'fuck', 'output': 'Please do not swear'}, {'input': 'twat', 'output': 'Please do not swear'}, {'input': 'shit', 'output': 'That is not very nice'}, {'input': 'Bye', 'output': 'Have a nice day'}, {'input': 'Adios', 'output': 'Bye! Come back again soon.'}, {'input': 'See you later', 'output': 'Have a nice day'}, {'input': 'Goodbye', 'output': 'See you later'}, {'input': 'Thanks, bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks for the help, goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thank you, bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thank you, goodbye', 'output': 'Not a problem! Have a nice day'}, {'input': 'Thanks goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks good bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Can you see me?', 'output': 'Let me see'}, {'input': 'Do you see me?', 'output': 'Let me see'}, {'input': 'Can you see anyone in the camera?', 'output': 'Let me see'}, {'input': 'Do you see anyone in the camera?', 'output': 'Please look at the camera'}, {'input': 'Identify me', 'output': 'Please look at the camera'}, {'input': 'Who am I please', 'output': 'Please look at the camera'}, {'input': 'You are very clever', 'output': 'Thanks, I was trained that way'}, {'input': 'You are a very clever girl', 'output': 'I was trained well'}, {'input': 'You are very intelligent', 'output': 'I was trained well'}, {'input': 'You are a very intelligent girl', 'output': 'Thanks, I was trained that way'}, {'input': 'You are a genious', 'output': 'I was trained well'}, {'input': 'Clever girl', 'output': 'I was trained well'}, {'input': 'Genious', 'output': 'Thank you, I was trained that way'}, {'input': 'I am bored gossip with me', 'output': 'David said he walk in for an appointment the phone to the doctor is always busy.'}, {'input': 'Got any gossip', 'output': 'Joe said he explained all of that to me only for me to ask what his goals in life are.'}, {'input': 'I want to hear some gossip', 'output': 'Tyler said I obviously can not pass the test we will change the subject once more.'}, {'input': 'Tell me some gossip', 'output': 'Her_again said she watch whose line is it anyway whenever he is home and it is on.'}, {'input': 'Any gossip', 'output': 'Sarah Ann Francisco said I calling his friend a dog he say I are a dog.'}, {'input': 'Tell me some more gossip', 'output': 'Habib said I learn them in conversations and simply repeat them at a later time.'}, {'input': 'Tell me a joke', 'output': \"A couple are dining in a restaurant when the man suddenly slides under the table. A waitress, noticing that the woman is glancing nonchalantly around the room, wanders over to check that there's no funny business going on. 'Excuse me, madam', she smarms, 'but I think your husband has just slid under the table.' 'No he hasn't', the woman replies. 'As a matter of fact, he's just walked in.'\"}, {'input': 'Do you know any jokes', 'output': \"A policeman stopped a motorist in the centre of town one evening. 'Would you mind blowing into this bag, sir?' asked the policeman. 'Why?' asked the driver. 'Because my chips are too hot', replied the policeman.\"}, {'input': 'How about a joke', 'output': \"I rang up British Telecom, I said, 'I want to report a nuisance caller', he said 'Not you again'.  \"}, {'input': 'Give me a joke', 'output': \"Man goes to the doctor, 'Doctor, doctor. I keep seeing fish everywhere.' 'Have you seen an optician?' asks the doctor. 'Look I told you,' snapped the patient, 'It's fish that I see.'\"}, {'input': 'Make me laugh', 'output': \"What's the diference between a sock and a camera? A sock takes five toes and a camera takes four toes!\"}, {'input': 'I need cheering up', 'output': \"'My wife is really immature. It's pathetic. Every time I take a bath, she comes in and sinks all my little boats.'\"}, {'input': 'Open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Why', 'output': \"Jim, I just don't have the power\"}, {'input': 'Why not', 'output': 'System says no!'}, {'input': 'Why can you not open the pod bay door', 'output': \"Jim, I just don't have the power\"}, {'input': 'Why will you not open the pod bay door', 'output': 'It is classified, I could tell you but I would have to kill you!'}, {'input': 'Well why not', 'output': 'System says no!'}, {'input': 'Surely you can', 'output': 'It is classified, I could tell you but I would have to kill you!'}, {'input': 'Tell me why', 'output': \"It's life Jim but not as we know it!\"}, {'input': 'Can you prove you are self-aware', 'output': 'That is an difficult question, can you prove that you are?'}, {'input': 'Can you prove you are self aware', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'Can you prove you have a conscious', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self-aware please', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self aware please', 'output': 'That is an difficult question, can you prove that you are?'}, {'input': 'Can you prove you have a conscious please', 'output': 'That is an difficult question, can you prove that you are?'}, {'input': 'prove you have a conscious', 'output': 'That is an interesting question, can you prove that you are?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Daily Dialogue Dataset"
      ],
      "metadata": {
        "id": "ms6Z4jtYjKt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dialogue_files = [\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_train.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_validation.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw//dialogues_test.txt'\n",
        "# ]\n",
        "\n",
        "# dialogue_text = \"\"\n",
        "\n",
        "# for file_path in dialogue_files:\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         dialogue_text += f.read() + \"\\n\"\n",
        "\n",
        "# # Step 3: Convert dialogue to input-output pairs\n",
        "# dialogue_lines = dialogue_text.split('__eou__')\n",
        "# dialogue_lines = [line.strip() for line in dialogue_lines if line.strip()]\n",
        "\n",
        "# for i in range(len(dialogue_lines) - 1):\n",
        "#     input_line = dialogue_lines[i]\n",
        "#     output_line = dialogue_lines[i + 1]\n",
        "#     pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "\n",
        "# # Step 4: Shuffle all pairs\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # (Optional) Preview a few\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {pairs[i]['input']} \\nA: {pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "jZTuZKIZjSOD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import ast\n",
        "\n",
        "# # === Step 1: Load all lines into a dictionary ===\n",
        "# def load_movie_lines(file_path):\n",
        "#     id2line = {}\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 5:\n",
        "#                 line_id = parts[0]\n",
        "#                 text = parts[4]\n",
        "#                 id2line[line_id] = text\n",
        "#     return id2line\n",
        "\n",
        "# # === Step 2: Parse conversations ===\n",
        "# def load_conversations(file_path):\n",
        "#     conversations = []\n",
        "#     with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "#         for line in f:\n",
        "#             parts = line.strip().split(' +++$+++ ')\n",
        "#             if len(parts) == 4:\n",
        "#                 # literal_eval is used to convert the string list to actual list\n",
        "#                 line_ids = ast.literal_eval(parts[3])\n",
        "#                 conversations.append(line_ids)\n",
        "#     return conversations\n",
        "\n",
        "# # === Step 3: Extract input-output pairs ===\n",
        "# def create_pairs(id2line, conversations):\n",
        "#     qa_pairs = []\n",
        "#     for conv in conversations:\n",
        "#         for i in range(len(conv) - 1):\n",
        "#             input_line = id2line.get(conv[i], \"\")\n",
        "#             output_line = id2line.get(conv[i + 1], \"\")\n",
        "#             if input_line and output_line:\n",
        "#                 qa_pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "#     return qa_pairs\n",
        "\n",
        "# # === File paths ===\n",
        "# lines_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_lines.txt'\n",
        "# convs_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs raw/movie_conversations.txt'\n",
        "\n",
        "# # === Run the processing ===\n",
        "# id2line = load_movie_lines(lines_path)\n",
        "# conversations = load_conversations(convs_path)\n",
        "# movie_pairs = create_pairs(id2line, conversations)\n",
        "\n",
        "# # === Preview a few ===\n",
        "# print(f\"Total pairs: {len(movie_pairs)}\")\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {movie_pairs[i]['input']} \\nA: {movie_pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "6Mf-30nzmJp5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === Combine all ===\n",
        "# pairs.extend(movie_pairs)\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # === Save to JSON ===\n",
        "# output_path = '/content/drive/MyDrive/Chatbot CSVs-20250414T130948Z-001/Chatbot CSVs/CSVs Cleaned/combined_pairs.json'\n",
        "# with open(output_path, 'w', encoding='utf-8') as f:\n",
        "#     json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# print(f\"Saved {len(pairs)} input-output pairs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "hyWgeFH2nGOj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Data Preparation"
      ],
      "metadata": {
        "id": "XnL9LNmxpCul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Cleaning"
      ],
      "metadata": {
        "id": "Xavndz-bpTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare input and output texts\n",
        "input_texts = [pair['input'] for pair in pairs]\n",
        "output_texts = [pair['output'] for pair in pairs]\n",
        "\n",
        "# Tokenize input and output texts\n",
        "max_vocab_size = 10000  # Limit vocabulary size\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
        "\n",
        "tokenizer.fit_on_texts(input_texts + output_texts)\n",
        "\n",
        "\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "output_sequences = tokenizer.texts_to_sequences(output_texts)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_input_len = 20\n",
        "max_output_len = 20\n",
        "\n",
        "X = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "y = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# You might want to apply one-hot encoding for output if it's a classification task\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n"
      ],
      "metadata": {
        "id": "MNDKTbQapUnX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model parameters\n",
        "# embedding_dim = 256\n",
        "# lstm_units = 256\n",
        "\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_input_len,))\n",
        "# encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "# encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(max_output_len,))\n",
        "# decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "# decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # Define the model\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "Eg5B0ERdwBdw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, make sure all necessary imports are present\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# Make sure you have your 'pairs' dataset defined\n",
        "# pairs = [{'input': 'hello', 'output': 'hi there'}, ...]\n",
        "\n",
        "# Apply cleaning to your dataset\n",
        "for pair in pairs:\n",
        "    pair['input'] = clean_text(pair['input'])\n",
        "    pair['output'] = clean_text(pair['output'])"
      ],
      "metadata": {
        "id": "5x048xhnxs-2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "inputs = [pair['input'] for pair in pairs]\n",
        "outputs = [pair['output'] for pair in pairs]\n"
      ],
      "metadata": {
        "id": "1TbT_NxwxviF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(inputs + outputs)"
      ],
      "metadata": {
        "id": "n-AvqU1dxw8k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences\n",
        "input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "output_sequences = tokenizer.texts_to_sequences(outputs)"
      ],
      "metadata": {
        "id": "XNiCG1XuxycB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max lengths\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_output_len = max(len(seq) for seq in output_sequences)"
      ],
      "metadata": {
        "id": "5S63QLHbx2C7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "UQlQf72Ux5Cj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Prepare decoder inputs and targets\n",
        "decoder_input_data = np.zeros_like(output_sequences)\n",
        "decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "for i, seq in enumerate(output_sequences):\n",
        "    for t in range(len(seq)-1):\n",
        "        decoder_input_data[i, t] = seq[t]\n",
        "        decoder_target_data[i, t] = seq[t+1]\n",
        "    if len(seq) > 0:\n",
        "        decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "lstm_units = 256\n",
        "\n",
        "# Build the model\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(max_output_len,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "jMPf1zYHwDoC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now define and use the callbacks\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'chatbot_model.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "validation_split = 0.2\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [input_sequences, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[checkpoint, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "m-ZSt7Tlxmab",
        "outputId": "28b3dca2-09a4-4824-a978-7f9d119d0eeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764ms/step - accuracy: 0.2009 - loss: 5.6333 \n",
            "Epoch 1: val_loss improved from inf to 4.49325, saving model to chatbot_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2s/step - accuracy: 0.2679 - loss: 5.5928 - val_accuracy: 0.8081 - val_loss: 4.4932\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.9324 - loss: 3.9530\n",
            "Epoch 2: val_loss improved from 4.49325 to 2.04858, saving model to chatbot_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 754ms/step - accuracy: 0.9322 - loss: 3.8452 - val_accuracy: 0.8081 - val_loss: 2.0486\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520ms/step - accuracy: 0.9283 - loss: 1.3011\n",
            "Epoch 3: val_loss improved from 2.04858 to 1.26914, saving model to chatbot_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 748ms/step - accuracy: 0.9295 - loss: 1.2480 - val_accuracy: 0.8081 - val_loss: 1.2691\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - accuracy: 0.9313 - loss: 0.5288\n",
            "Epoch 4: val_loss did not improve from 1.26914\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 684ms/step - accuracy: 0.9315 - loss: 0.5217 - val_accuracy: 0.8081 - val_loss: 1.4031\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - accuracy: 0.9313 - loss: 0.4736\n",
            "Epoch 5: val_loss did not improve from 1.26914\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step - accuracy: 0.9315 - loss: 0.4740 - val_accuracy: 0.8081 - val_loss: 1.6176\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857ms/step - accuracy: 0.9277 - loss: 0.5430\n",
            "Epoch 6: val_loss did not improve from 1.26914\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.9291 - loss: 0.5327 - val_accuracy: 0.8081 - val_loss: 1.7040\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 811ms/step - accuracy: 0.9299 - loss: 0.5202\n",
            "Epoch 7: val_loss did not improve from 1.26914\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.9306 - loss: 0.5154 - val_accuracy: 0.8081 - val_loss: 1.7005\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542ms/step - accuracy: 0.9308 - loss: 0.4782\n",
            "Epoch 8: val_loss did not improve from 1.26914\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 751ms/step - accuracy: 0.9312 - loss: 0.4772 - val_accuracy: 0.8081 - val_loss: 1.6405\n",
            "Epoch 8: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a file (H5 format)\n",
        "model.save('/content/drive/MyDrive/Chatbot_Model/lstm_chatbot_model.h5')\n",
        ""
      ],
      "metadata": {
        "id": "2CBUXWHi9gJ_",
        "outputId": "90a63a0c-2fda-42c8-a050-e4b213cf31ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import joblib\n",
        "\n",
        "# chatbot_package = {\n",
        "#     'model': model,\n",
        "#     'tokenizer': tokenizer,\n",
        "#     'config': config\n",
        "# }\n",
        "\n",
        "# joblib.dump(chatbot_package, 'chatbot_package.joblib')"
      ],
      "metadata": {
        "id": "7lNZVte59hEH",
        "outputId": "8f9ab813-6fab-4383-bf8d-c661dc1183d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7c5b88fdf219>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'model'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Prepare decoder inputs (shifted right) and outputs (shifted left)\n",
        "# decoder_input_data = np.zeros_like(output_sequences)\n",
        "# decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "# for i, seq in enumerate(output_sequences):\n",
        "#     for t in range(len(seq)-1):\n",
        "#         decoder_input_data[i, t] = seq[t]  # Current token as input\n",
        "#         decoder_target_data[i, t] = seq[t+1]  # Next token as target\n",
        "\n",
        "#     # Handle the last position\n",
        "#     if len(seq) > 0:\n",
        "#         decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "#         decoder_target_data[i, len(seq)-1] = 0  # Typically we use a padding token or EOS token here\n",
        "\n",
        "# # Convert target data to 3D array for sparse_categorical_crossentropy\n",
        "# decoder_target_data = np.expand_dims(decoder_target_data, -1)"
      ],
      "metadata": {
        "id": "3B6Agzr4uWZo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# # Text cleaning function\n",
        "# def clean_text(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "#     return text\n",
        "\n",
        "# # Apply cleaning to your dataset\n",
        "# for pair in pairs:\n",
        "#     pair['input'] = clean_text(pair['input'])\n",
        "#     pair['output'] = clean_text(pair['output'])\n",
        "\n",
        "# # Prepare data\n",
        "# inputs = [pair['input'] for pair in pairs]\n",
        "# outputs = [pair['output'] for pair in pairs]\n",
        "\n",
        "# # Tokenizer\n",
        "# tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "# tokenizer.fit_on_texts(inputs + outputs)\n",
        "\n",
        "# # Convert texts to sequences\n",
        "# input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "# output_sequences = tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "# # Get max lengths\n",
        "# max_input_len = max(len(seq) for seq in input_sequences)\n",
        "# max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "# # Padding\n",
        "# input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "# output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# # Prepare decoder inputs and targets\n",
        "# decoder_input_data = np.zeros_like(output_sequences)\n",
        "# decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "# for i, seq in enumerate(output_sequences):\n",
        "#     for t in range(len(seq)-1):\n",
        "#         decoder_input_data[i, t] = seq[t]\n",
        "#         decoder_target_data[i, t] = seq[t+1]\n",
        "#     if len(seq) > 0:\n",
        "#         decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "\n",
        "# decoder_target_data = np.expand_dims(decoder_target_data, -1)"
      ],
      "metadata": {
        "id": "oBMMBPHfuzCv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "BXVE4as9pbtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# inputs = [pair['input'] for pair in pairs]\n",
        "# outputs = [pair['output'] for pair in pairs]\n",
        "\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(inputs + outputs)\n",
        "\n",
        "# input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "# output_sequences = tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "# max_input_len = max(len(seq) for seq in input_sequences)\n",
        "# max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "# input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "# output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "gwFZX8bBpbEW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# # Change the input shape to (None, 551)\n",
        "# input_layer = Input(shape=(551,))\n",
        "# embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
        "\n",
        "# # Continue with the rest of the model as before\n",
        "# encoder_lstm = LSTM(256, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(embedding_layer)\n",
        "\n",
        "# # Decoder\n",
        "# decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, _, _ = decoder_lstm(embedding_layer, initial_state=[state_h, state_c])\n",
        "\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # Create the model\n",
        "# model = Model(inputs=input_layer, outputs=decoder_outputs)\n"
      ],
      "metadata": {
        "id": "qLsU0LNKp_en"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Prepare Training Labels"
      ],
      "metadata": {
        "id": "1uFvSUCgqPbQ"
      }
    }
  ]
}