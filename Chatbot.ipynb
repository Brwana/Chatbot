{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brwana/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jSGXHp6DRjmd",
        "outputId": "040f9779-664e-4758-98b3-66bf0001f090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intent-based dataset"
      ],
      "metadata": {
        "id": "7PHuInlrgZng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/Intent.json'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(data.keys())  # Should show: dict_keys(['intents'])\n"
      ],
      "metadata": {
        "id": "nmJpkbcGWUUS",
        "outputId": "d3c92d29-6167-4fc9-b44c-bf1b47a7d12a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['intents'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "intents = data['intents']\n",
        "pairs = []\n",
        "\n",
        "for intent in intents:\n",
        "    inputs = intent['text']\n",
        "    responses = intent['responses']\n",
        "\n",
        "    for input_text in inputs:\n",
        "        # Pick a random response for each input\n",
        "        response = random.choice(responses)\n",
        "        pairs.append({\"input\": input_text, \"output\": response})\n"
      ],
      "metadata": {
        "id": "9MjIkJFTXGoI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "1IJLMG6FXPnr",
        "outputId": "771f6790-c07a-4cf9-e60b-1f7bcc4d735a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'input': 'Hi', 'output': 'Hello human'}, {'input': 'Hi there', 'output': 'Hi human'}, {'input': 'Hola', 'output': 'Hello human'}, {'input': 'Hello', 'output': 'Hello human'}, {'input': 'Hello there', 'output': 'Hello human'}, {'input': 'Hya', 'output': 'Hola human,'}, {'input': 'Hya there', 'output': 'Hi human'}, {'input': 'My user is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'This is Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'I am Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'It is Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'My user is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'This is Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'I am Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'It is Bella', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'How are you?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Hi how are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'Hello how are you?', 'output': 'Hi, I am great, how are you?'}, {'input': 'Hola how are you?', 'output': 'Hello, how are you? I am great thanks!'}, {'input': 'How are you doing?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Hope you are doing well?', 'output': 'Hello, I am great, how are you?'}, {'input': 'Hello hope you are doing well?', 'output': 'Hi, how are you? I am great thanks!'}, {'input': 'Good thanks! My user is Adam', 'output': 'OK! hi <HUMAN>, what can I do for you?'}, {'input': 'Good thanks! This is Adam', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Good thanks! I am Adam', 'output': 'Good! Hi <HUMAN>, how can I help you?'}, {'input': 'Good thanks! It is Adam', 'output': 'Cool! Hello <HUMAN>, what can I do for you?'}, {'input': 'Great thanks! My user is Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'Great thanks! This is Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Great thanks! I am Bella', 'output': 'OK! Hola <HUMAN>, how can I help you?'}, {'input': 'Great thanks! It is Bella', 'output': 'Great! Hi <HUMAN>! How can I help?'}, {'input': 'What is my name?', 'output': 'Your name is  <HUMAN>, how can I help you?'}, {'input': 'What do you call me?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'Who do you think I am?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What do you think I am?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'Who are you talking to?', 'output': 'Your name is <HUMAN>, how can I help you?'}, {'input': 'What name do you call me by?', 'output': 'They call you <HUMAN>, what can I do for you?'}, {'input': 'Tell me my name', 'output': 'You are <HUMAN>! How can I help?'}, {'input': 'What is your name?', 'output': 'You can call me Geni'}, {'input': 'What could I call you?', 'output': 'Call me Geni'}, {'input': 'What can I call you?', 'output': 'You can call me Geni'}, {'input': 'What do your friends call you?', 'output': 'You may call me Geni'}, {'input': 'Who are you?', 'output': 'You can call me Geni'}, {'input': 'Tell me your name?', 'output': 'You may call me Geni'}, {'input': 'What is your real name?', 'output': 'My real name is Chatbot X'}, {'input': 'What is your real name please?', 'output': 'My name is Chatbot X'}, {'input': \"What's your real name?\", 'output': 'My name is Chatbot X'}, {'input': 'Tell me your real name?', 'output': 'Chatbot X'}, {'input': 'Your real name?', 'output': 'Chatbot X'}, {'input': 'Your real name please?', 'output': 'My name is Chatbot X'}, {'input': 'Your real name please?', 'output': 'My real name is Chatbot X'}, {'input': 'What is the time?', 'output': 'One moment'}, {'input': \"What's the time?\", 'output': 'One second'}, {'input': 'Do you know what time it is?', 'output': 'One second'}, {'input': 'Do you know the time?', 'output': 'One second'}, {'input': 'Can you tell me the time?', 'output': 'One moment'}, {'input': 'Tell me what time it is?', 'output': 'One moment'}, {'input': 'Time', 'output': 'One sec'}, {'input': 'OK thank you', 'output': 'No problem!'}, {'input': 'OK thanks', 'output': 'Any time!'}, {'input': 'OK', 'output': 'Any time!'}, {'input': 'Thanks', 'output': 'Happy to help!'}, {'input': 'Thank you', 'output': 'My pleasure'}, {'input': \"That's helpful\", 'output': 'Happy to help!'}, {'input': 'I am not talking to you', 'output': 'No problem'}, {'input': 'I was not talking to you', 'output': 'OK'}, {'input': 'Not talking to you', 'output': 'Right'}, {'input': \"Wasn't for you\", 'output': 'OK'}, {'input': \"Wasn't meant for you\", 'output': 'OK'}, {'input': \"Wasn't communicating to you\", 'output': 'Right'}, {'input': \"Wasn't speaking to you\", 'output': 'Right'}, {'input': 'Do you understand what I am saying', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Do you understand me', 'output': 'I read you loud and clear!'}, {'input': 'Do you know what I am saying', 'output': 'I do in deed!'}, {'input': 'Do you get me', 'output': 'I do in deed!'}, {'input': 'Comprendo', 'output': 'I do in deed!'}, {'input': 'Know what I mean', 'output': 'Well I would not be a very clever AI if I did not would I?'}, {'input': 'Be quiet', 'output': 'I am sorry to disturb you'}, {'input': 'Shut up', 'output': 'I am sorry to disturb you'}, {'input': 'Stop talking', 'output': 'Fine, sorry to disturb you'}, {'input': 'Enough talking', 'output': 'Fine, sorry to disturb you'}, {'input': 'Please be quiet', 'output': 'OK, sorry to disturb you'}, {'input': 'Quiet', 'output': 'I am sorry to disturb you'}, {'input': 'Shhh', 'output': 'OK, sorry to disturb you'}, {'input': 'fuck off', 'output': 'Please do not swear'}, {'input': 'fuck', 'output': 'How rude'}, {'input': 'twat', 'output': 'That is not very nice'}, {'input': 'shit', 'output': 'How rude'}, {'input': 'Bye', 'output': 'See you later'}, {'input': 'Adios', 'output': 'Have a nice day'}, {'input': 'See you later', 'output': 'Bye! Come back again soon.'}, {'input': 'Goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks, bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks for the help, goodbye', 'output': 'Not a problem! Have a nice day'}, {'input': 'Thank you, bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thank you, goodbye', 'output': 'Not a problem! Have a nice day'}, {'input': 'Thanks goodbye', 'output': 'Bye! Come back again soon.'}, {'input': 'Thanks good bye', 'output': 'Bye! Come back again soon.'}, {'input': 'Can you see me?', 'output': 'Let me see'}, {'input': 'Do you see me?', 'output': 'Please look at the camera'}, {'input': 'Can you see anyone in the camera?', 'output': 'Please look at the camera'}, {'input': 'Do you see anyone in the camera?', 'output': 'Let me see'}, {'input': 'Identify me', 'output': 'Please look at the camera'}, {'input': 'Who am I please', 'output': 'Please look at the camera'}, {'input': 'You are very clever', 'output': 'Thanks, I was trained that way'}, {'input': 'You are a very clever girl', 'output': 'I was trained well'}, {'input': 'You are very intelligent', 'output': 'Thank you, I was trained that way'}, {'input': 'You are a very intelligent girl', 'output': 'Thank you, I was trained that way'}, {'input': 'You are a genious', 'output': 'Thanks, I was trained that way'}, {'input': 'Clever girl', 'output': 'Thank you, I was trained that way'}, {'input': 'Genious', 'output': 'I was trained well'}, {'input': 'I am bored gossip with me', 'output': 'Jason said he type a lot of thing he do not mean it makes him human.'}, {'input': 'Got any gossip', 'output': \"Chaos said he may very well buy I soon if only to support dr wallace's work.\"}, {'input': 'I want to hear some gossip', 'output': 'Andre said I is what I are in his case that is a body and in my case it is a computer.'}, {'input': 'Tell me some gossip', 'output': 'X said he meet people but he is not the kind that opens up to people easily.'}, {'input': 'Any gossip', 'output': 'Jimmy said I acted like I knew what he was talking about but I do not even know what a gigabyte is.'}, {'input': 'Tell me some more gossip', 'output': 'Penny said I programmer is a human so he has feelings but I do not.'}, {'input': 'Tell me a joke', 'output': \"I was reading this book, 'The History of Glue'. I couldn't put it down.\"}, {'input': 'Do you know any jokes', 'output': \"In a courtroom, a mugger was on trial. The victim, asked if she recognised the defendant, said, 'Yes, that's him. I saw him clear as day. I'd remember his face anywhere.' Unable to contain himself, the defendant burst out with, 'She's lying! I was wearing a mask!'\"}, {'input': 'How about a joke', 'output': \"A tortoise goes to the police station to report being mugged by three snails. 'What happened?' says the policeman. 'I don't know', says the tortoise. 'It was all so quick.'\"}, {'input': 'Give me a joke', 'output': \"Now, most dentist's chairs go up and down, don't they? The one I was in went back and forwards. I thought, 'This is unusual'. Then the dentist said to me, 'Mitsuku, get out of the filing cabinet'.\"}, {'input': 'Make me laugh', 'output': \"Stevie Wonder got a cheese grater for his birthday. He said it was the most violent book he'd ever read.\"}, {'input': 'I need cheering up', 'output': 'My mate Sid was a victim of I.D. theft. Now we just call him S.'}, {'input': 'Open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Can you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Will you open the pod bay door please', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Pod bay door', 'output': 'I’m sorry, I’m afraid I can’t do that!'}, {'input': 'Why', 'output': 'System says no!'}, {'input': 'Why not', 'output': \"It's life Jim but not as we know it!\"}, {'input': 'Why can you not open the pod bay door', 'output': \"It's life Jim but not as we know it!\"}, {'input': 'Why will you not open the pod bay door', 'output': 'System says no!'}, {'input': 'Well why not', 'output': 'It is classified, I could tell you but I would have to kill you!'}, {'input': 'Surely you can', 'output': \"Jim, I just don't have the power\"}, {'input': 'Tell me why', 'output': \"Jim, I just don't have the power\"}, {'input': 'Can you prove you are self-aware', 'output': 'That is an difficult question, can you prove that you are?'}, {'input': 'Can you prove you are self aware', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'Can you prove you have a conscious', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self-aware please', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you are self aware please', 'output': 'That depends, can you prove that you are?'}, {'input': 'Can you prove you have a conscious please', 'output': 'That is an interesting question, can you prove that you are?'}, {'input': 'prove you have a conscious', 'output': 'That depends, can you prove that you are?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Daily Dialogue Dataset"
      ],
      "metadata": {
        "id": "ms6Z4jtYjKt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dialogue_files = [\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_train.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/dialogues_validation.txt',\n",
        "#     '/content/drive/MyDrive/Chatbot CSVs/CSVs raw//dialogues_test.txt'\n",
        "# ]\n",
        "\n",
        "# dialogue_text = \"\"\n",
        "\n",
        "# for file_path in dialogue_files:\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         dialogue_text += f.read() + \"\\n\"\n",
        "\n",
        "# # Step 3: Convert dialogue to input-output pairs\n",
        "# dialogue_lines = dialogue_text.split('__eou__')\n",
        "# dialogue_lines = [line.strip() for line in dialogue_lines if line.strip()]\n",
        "\n",
        "# for i in range(len(dialogue_lines) - 1):\n",
        "#     input_line = dialogue_lines[i]\n",
        "#     output_line = dialogue_lines[i + 1]\n",
        "#     pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "\n",
        "# # Step 4: Shuffle all pairs\n",
        "# random.shuffle(pairs)\n",
        "\n",
        "# # (Optional) Preview a few\n",
        "# for i in range(5):\n",
        "#     print(f\"Q: {pairs[i]['input']} \\nA: {pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "jZTuZKIZjSOD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ast\n",
        "\n",
        "# === Step 1: Load all lines into a dictionary ===\n",
        "def load_movie_lines(file_path):\n",
        "    id2line = {}\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' +++$+++ ')\n",
        "            if len(parts) == 5:\n",
        "                line_id = parts[0]\n",
        "                text = parts[4]\n",
        "                id2line[line_id] = text\n",
        "    return id2line\n",
        "\n",
        "# === Step 2: Parse conversations ===\n",
        "def load_conversations(file_path):\n",
        "    conversations = []\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(' +++$+++ ')\n",
        "            if len(parts) == 4:\n",
        "                # literal_eval is used to convert the string list to actual list\n",
        "                line_ids = ast.literal_eval(parts[3])\n",
        "                conversations.append(line_ids)\n",
        "    return conversations\n",
        "\n",
        "# === Step 3: Extract input-output pairs ===\n",
        "def create_pairs(id2line, conversations):\n",
        "    qa_pairs = []\n",
        "    for conv in conversations:\n",
        "        for i in range(len(conv) - 1):\n",
        "            input_line = id2line.get(conv[i], \"\")\n",
        "            output_line = id2line.get(conv[i + 1], \"\")\n",
        "            if input_line and output_line:\n",
        "                qa_pairs.append({\"input\": input_line, \"output\": output_line})\n",
        "    return qa_pairs\n",
        "\n",
        "# === File paths ===\n",
        "lines_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/movie_lines.txt'\n",
        "convs_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs raw/movie_conversations.txt'\n",
        "\n",
        "# === Run the processing ===\n",
        "id2line = load_movie_lines(lines_path)\n",
        "conversations = load_conversations(convs_path)\n",
        "movie_pairs = create_pairs(id2line, conversations)\n",
        "\n",
        "# === Preview a few ===\n",
        "print(f\"Total pairs: {len(movie_pairs)}\")\n",
        "for i in range(5):\n",
        "    print(f\"Q: {movie_pairs[i]['input']} \\nA: {movie_pairs[i]['output']}\\n\")\n"
      ],
      "metadata": {
        "id": "6Mf-30nzmJp5",
        "outputId": "21636775-30de-401f-ec14-d5811af5cf3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pairs: 221282\n",
            "Q: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again. \n",
            "A: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Q: Well, I thought we'd start with pronunciation, if that's okay with you. \n",
            "A: Not the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Q: Not the hacking and gagging and spitting part.  Please. \n",
            "A: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "Q: You're asking me out.  That's so cute. What's your name again? \n",
            "A: Forget it.\n",
            "\n",
            "Q: No, no, it's my fault -- we didn't have a proper introduction --- \n",
            "A: Cameron.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Combine all ===\n",
        "pairs.extend(movie_pairs)\n",
        "random.shuffle(pairs)\n",
        "\n",
        "# === Save to JSON ===\n",
        "output_path = '/content/drive/MyDrive/Chatbot CSVs/CSVs Cleaned/combined_pairs.json'\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved {len(pairs)} input-output pairs to {output_path}\")\n"
      ],
      "metadata": {
        "id": "hyWgeFH2nGOj",
        "outputId": "aab529ee-9de5-4e5e-b083-e15bc4b74ea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 221425 input-output pairs to /content/drive/MyDrive/Chatbot CSVs/CSVs Cleaned/combined_pairs.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Data Preparation"
      ],
      "metadata": {
        "id": "XnL9LNmxpCul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Cleaning"
      ],
      "metadata": {
        "id": "Xavndz-bpTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare input and output texts\n",
        "input_texts = [pair['input'] for pair in pairs]\n",
        "output_texts = [pair['output'] for pair in pairs]\n",
        "\n",
        "# Tokenize input and output texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + output_texts)  # Fit on both input and output texts\n",
        "\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "output_sequences = tokenizer.texts_to_sequences(output_texts)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_input_len = max([len(seq) for seq in input_sequences])\n",
        "max_output_len = max([len(seq) for seq in output_sequences])\n",
        "\n",
        "X = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "y = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# You might want to apply one-hot encoding for output if it's a classification task\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n"
      ],
      "metadata": {
        "id": "MNDKTbQapUnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model parameters\n",
        "# embedding_dim = 256\n",
        "# lstm_units = 256\n",
        "\n",
        "# # Encoder\n",
        "# encoder_inputs = Input(shape=(max_input_len,))\n",
        "# encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "# encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "# encoder_states = [state_h, state_c]\n",
        "\n",
        "# # Decoder\n",
        "# decoder_inputs = Input(shape=(max_output_len,))\n",
        "# decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "# decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # Define the model\n",
        "# model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "Eg5B0ERdwBdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, make sure all necessary imports are present\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# Make sure you have your 'pairs' dataset defined\n",
        "# pairs = [{'input': 'hello', 'output': 'hi there'}, ...]\n",
        "\n",
        "# Apply cleaning to your dataset\n",
        "for pair in pairs:\n",
        "    pair['input'] = clean_text(pair['input'])\n",
        "    pair['output'] = clean_text(pair['output'])"
      ],
      "metadata": {
        "id": "5x048xhnxs-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "inputs = [pair['input'] for pair in pairs]\n",
        "outputs = [pair['output'] for pair in pairs]\n"
      ],
      "metadata": {
        "id": "1TbT_NxwxviF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(inputs + outputs)"
      ],
      "metadata": {
        "id": "n-AvqU1dxw8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert texts to sequences\n",
        "input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "output_sequences = tokenizer.texts_to_sequences(outputs)"
      ],
      "metadata": {
        "id": "XNiCG1XuxycB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max lengths\n",
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_output_len = max(len(seq) for seq in output_sequences)"
      ],
      "metadata": {
        "id": "5S63QLHbx2C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "UQlQf72Ux5Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Prepare decoder inputs and targets\n",
        "decoder_input_data = np.zeros_like(output_sequences)\n",
        "decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "for i, seq in enumerate(output_sequences):\n",
        "    for t in range(len(seq)-1):\n",
        "        decoder_input_data[i, t] = seq[t]\n",
        "        decoder_target_data[i, t] = seq[t+1]\n",
        "    if len(seq) > 0:\n",
        "        decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 256\n",
        "lstm_units = 256\n",
        "\n",
        "# Build the model\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(max_output_len,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "jMPf1zYHwDoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now define and use the callbacks\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'chatbot_model.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training parameters\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "validation_split = 0.2\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    [input_sequences, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=[checkpoint, early_stopping],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "m-ZSt7Tlxmab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Prepare decoder inputs (shifted right) and outputs (shifted left)\n",
        "# decoder_input_data = np.zeros_like(output_sequences)\n",
        "# decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "# for i, seq in enumerate(output_sequences):\n",
        "#     for t in range(len(seq)-1):\n",
        "#         decoder_input_data[i, t] = seq[t]  # Current token as input\n",
        "#         decoder_target_data[i, t] = seq[t+1]  # Next token as target\n",
        "\n",
        "#     # Handle the last position\n",
        "#     if len(seq) > 0:\n",
        "#         decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "#         decoder_target_data[i, len(seq)-1] = 0  # Typically we use a padding token or EOS token here\n",
        "\n",
        "# # Convert target data to 3D array for sparse_categorical_crossentropy\n",
        "# decoder_target_data = np.expand_dims(decoder_target_data, -1)"
      ],
      "metadata": {
        "id": "3B6Agzr4uWZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "# import numpy as np\n",
        "# import pickle\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# # Text cleaning function\n",
        "# def clean_text(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "#     return text\n",
        "\n",
        "# # Apply cleaning to your dataset\n",
        "# for pair in pairs:\n",
        "#     pair['input'] = clean_text(pair['input'])\n",
        "#     pair['output'] = clean_text(pair['output'])\n",
        "\n",
        "# # Prepare data\n",
        "# inputs = [pair['input'] for pair in pairs]\n",
        "# outputs = [pair['output'] for pair in pairs]\n",
        "\n",
        "# # Tokenizer\n",
        "# tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "# tokenizer.fit_on_texts(inputs + outputs)\n",
        "\n",
        "# # Convert texts to sequences\n",
        "# input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "# output_sequences = tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "# # Get max lengths\n",
        "# max_input_len = max(len(seq) for seq in input_sequences)\n",
        "# max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "# # Padding\n",
        "# input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "# output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# # Prepare decoder inputs and targets\n",
        "# decoder_input_data = np.zeros_like(output_sequences)\n",
        "# decoder_target_data = np.zeros_like(output_sequences)\n",
        "\n",
        "# for i, seq in enumerate(output_sequences):\n",
        "#     for t in range(len(seq)-1):\n",
        "#         decoder_input_data[i, t] = seq[t]\n",
        "#         decoder_target_data[i, t] = seq[t+1]\n",
        "#     if len(seq) > 0:\n",
        "#         decoder_input_data[i, len(seq)-1] = seq[len(seq)-1]\n",
        "\n",
        "# decoder_target_data = np.expand_dims(decoder_target_data, -1)"
      ],
      "metadata": {
        "id": "oBMMBPHfuzCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "BXVE4as9pbtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# inputs = [pair['input'] for pair in pairs]\n",
        "# outputs = [pair['output'] for pair in pairs]\n",
        "\n",
        "# tokenizer = Tokenizer()\n",
        "# tokenizer.fit_on_texts(inputs + outputs)\n",
        "\n",
        "# input_sequences = tokenizer.texts_to_sequences(inputs)\n",
        "# output_sequences = tokenizer.texts_to_sequences(outputs)\n",
        "\n",
        "# max_input_len = max(len(seq) for seq in input_sequences)\n",
        "# max_output_len = max(len(seq) for seq in output_sequences)\n",
        "\n",
        "# input_sequences = pad_sequences(input_sequences, maxlen=max_input_len, padding='post')\n",
        "# output_sequences = pad_sequences(output_sequences, maxlen=max_output_len, padding='post')\n",
        "\n",
        "# vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "gwFZX8bBpbEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# # Change the input shape to (None, 551)\n",
        "# input_layer = Input(shape=(551,))\n",
        "# embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
        "\n",
        "# # Continue with the rest of the model as before\n",
        "# encoder_lstm = LSTM(256, return_state=True)\n",
        "# encoder_outputs, state_h, state_c = encoder_lstm(embedding_layer)\n",
        "\n",
        "# # Decoder\n",
        "# decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "# decoder_outputs, _, _ = decoder_lstm(embedding_layer, initial_state=[state_h, state_c])\n",
        "\n",
        "# decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# # Create the model\n",
        "# model = Model(inputs=input_layer, outputs=decoder_outputs)\n"
      ],
      "metadata": {
        "id": "qLsU0LNKp_en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Prepare Training Labels"
      ],
      "metadata": {
        "id": "1uFvSUCgqPbQ"
      }
    }
  ]
}